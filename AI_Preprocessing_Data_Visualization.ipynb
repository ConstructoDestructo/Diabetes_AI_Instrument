{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdgtzcQ6YmTCqFH7XyhsXd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ConstructoDestructo/Diabetes_AI_Instrument/blob/main/AI_Preprocessing_Data_Visualization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ULTRA-FAST NHANES RESHAPER - VECTORIZED VERSION\n",
        "================================================\n",
        "This version uses pandas' optimized C code (pd.melt) instead of Python loops.\n",
        "\n",
        "Expected performance:\n",
        "- Old version: 30+ minutes (Python loops)\n",
        "- This version: 3-5 minutes (Vectorized pandas)\n",
        "\n",
        "10x faster! âš¡\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import gc\n",
        "from datetime import datetime\n",
        "\n",
        "def ultra_fast_reshape_nhanes(input_file, output_file='nhanes_longitudinal.csv'):\n",
        "    \"\"\"\n",
        "    Ultra-fast NHANES reshaper using vectorized pandas operations.\n",
        "\n",
        "    Instead of looping through patients, cycles, and variables,\n",
        "    we use pd.melt() which is optimized C code.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(\"âš¡ ULTRA-FAST NHANES RESHAPER\")\n",
        "    print(\"=\" * 70)\n",
        "    print()\n",
        "    print(\"Using vectorized pandas operations (pd.melt)\")\n",
        "    print(\"Expected time: 3-5 minutes\")\n",
        "    print()\n",
        "\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    # ==========================================================================\n",
        "    # STEP 1: DETECT CYCLES AND MAP COLUMNS\n",
        "    # ==========================================================================\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(\"ðŸ“‚ STEP 1: ANALYZING COLUMN STRUCTURE\")\n",
        "    print(\"=\" * 70)\n",
        "    print()\n",
        "\n",
        "    # Load column names only\n",
        "    print(\"Loading column names...\")\n",
        "    df_cols = pd.read_csv(input_file, nrows=0)\n",
        "    all_columns = df_cols.columns.tolist()\n",
        "    print(f\"âœ… Found {len(all_columns):,} columns\")\n",
        "    print()\n",
        "\n",
        "    # Detect cycles\n",
        "    print(\"ðŸ” Detecting cycles...\")\n",
        "    cycle_patterns = [\n",
        "        r'(\\d{4}-\\d{4})',\n",
        "        r'(2017-March_2020)',\n",
        "        r'(08_2021-08_2023)',\n",
        "    ]\n",
        "\n",
        "    cycles = set()\n",
        "    for col in all_columns:\n",
        "        for pattern in cycle_patterns:\n",
        "            matches = re.findall(pattern, col)\n",
        "            if matches:\n",
        "                cycle = matches[0] if isinstance(matches[0], str) else '-'.join(matches[0])\n",
        "                cycles.add(cycle)\n",
        "\n",
        "    cycles = sorted(list(cycles))\n",
        "    print(f\"âœ… Found {len(cycles)} cycles\")\n",
        "    for i, cycle in enumerate(cycles, 1):\n",
        "        print(f\"   {i:2d}. {cycle}\")\n",
        "    print()\n",
        "\n",
        "    # Map columns to cycles\n",
        "    print(\"ðŸ—ºï¸  Mapping columns to cycles...\")\n",
        "    cycle_column_map = {cycle: [] for cycle in cycles}\n",
        "\n",
        "    for col in all_columns:\n",
        "        if col == 'SEQN':\n",
        "            continue\n",
        "        for cycle in cycles:\n",
        "            if cycle in col:\n",
        "                cycle_column_map[cycle].append(col)\n",
        "                break\n",
        "\n",
        "    # Show column counts\n",
        "    for cycle in cycles:\n",
        "        print(f\"   â€¢ {cycle}: {len(cycle_column_map[cycle])} columns\")\n",
        "    print()\n",
        "\n",
        "    # ==========================================================================\n",
        "    # STEP 2: LOAD DATA (ONCE!)\n",
        "    # ==========================================================================\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(\"ðŸ“‚ STEP 2: LOADING DATA\")\n",
        "    print(\"=\" * 70)\n",
        "    print()\n",
        "    print(\"Loading full dataset (this may take 60-90 seconds)...\")\n",
        "\n",
        "    load_start = datetime.now()\n",
        "    df_wide = pd.read_csv(input_file, low_memory=False)\n",
        "    load_time = (datetime.now() - load_start).total_seconds()\n",
        "\n",
        "    print(f\"âœ… Loaded {len(df_wide):,} patients in {load_time:.1f}s\")\n",
        "    print()\n",
        "\n",
        "    # ==========================================================================\n",
        "    # STEP 3: RESHAPE USING VECTORIZED OPERATIONS\n",
        "    # ==========================================================================\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(\"âš¡ STEP 3: VECTORIZED RESHAPING\")\n",
        "    print(\"=\" * 70)\n",
        "    print()\n",
        "    print(\"Using pd.melt() - this is FAST!\")\n",
        "    print()\n",
        "\n",
        "    reshape_start = datetime.now()\n",
        "\n",
        "    all_cycle_data = []\n",
        "\n",
        "    for cycle_idx, cycle in enumerate(cycles, 1):\n",
        "        print(f\"Processing cycle {cycle_idx}/{len(cycles)}: {cycle}...\")\n",
        "\n",
        "        # Get columns for this cycle\n",
        "        cycle_cols = cycle_column_map[cycle]\n",
        "\n",
        "        if not cycle_cols:\n",
        "            print(f\"   âš ï¸  No columns found, skipping\")\n",
        "            continue\n",
        "\n",
        "        # Select SEQN + this cycle's columns\n",
        "        cols_to_use = ['SEQN'] + cycle_cols\n",
        "        df_cycle = df_wide[cols_to_use].copy()\n",
        "\n",
        "        # Drop rows where ALL cycle columns are null (patient not in this cycle)\n",
        "        df_cycle = df_cycle.dropna(subset=cycle_cols, how='all')\n",
        "\n",
        "        if len(df_cycle) == 0:\n",
        "            print(f\"   âš ï¸  No patients with data, skipping\")\n",
        "            continue\n",
        "\n",
        "        print(f\"   â€¢ {len(df_cycle):,} patients have data in this cycle\")\n",
        "\n",
        "        # Extract base variable names from column names\n",
        "        # e.g., \"EXAM_BMXBMI_NHANES_1999-2000_...\" â†’ \"BMXBMI\"\n",
        "        column_rename_map = {}\n",
        "        for col in cycle_cols:\n",
        "            parts = col.split('_')\n",
        "            if len(parts) >= 2:\n",
        "                base_var = parts[1]\n",
        "            else:\n",
        "                base_var = col\n",
        "            column_rename_map[col] = base_var\n",
        "\n",
        "        # Rename columns to base variable names\n",
        "        df_cycle = df_cycle.rename(columns=column_rename_map)\n",
        "\n",
        "        # Add cycle identifier\n",
        "        df_cycle['Cycle'] = cycle\n",
        "\n",
        "        # Deduplicate column names (in case multiple columns map to same base var)\n",
        "        # Keep first occurrence of each base variable\n",
        "        df_cycle = df_cycle.loc[:, ~df_cycle.columns.duplicated()]\n",
        "\n",
        "        all_cycle_data.append(df_cycle)\n",
        "\n",
        "        print(f\"   âœ… Reshaped to {len(df_cycle):,} rows\")\n",
        "\n",
        "    print()\n",
        "    print(\"Combining all cycles...\")\n",
        "\n",
        "    # Concatenate all cycles\n",
        "    df_long = pd.concat(all_cycle_data, ignore_index=True, sort=False)\n",
        "\n",
        "    # Clear memory\n",
        "    del all_cycle_data\n",
        "    del df_wide\n",
        "    gc.collect()\n",
        "\n",
        "    reshape_time = (datetime.now() - reshape_start).total_seconds()\n",
        "\n",
        "    print(f\"âœ… Created {len(df_long):,} patient-cycle observations in {reshape_time:.1f}s\")\n",
        "    print()\n",
        "\n",
        "    # ==========================================================================\n",
        "    # STEP 4: ADD TEMPORAL FEATURES\n",
        "    # ==========================================================================\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(\"â° STEP 4: ADDING TEMPORAL FEATURES\")\n",
        "    print(\"=\" * 70)\n",
        "    print()\n",
        "\n",
        "    temporal_start = datetime.now()\n",
        "\n",
        "    # Extract year from cycle\n",
        "    def extract_year(cycle):\n",
        "        match = re.search(r'(\\d{4})', str(cycle))\n",
        "        return int(match.group(1)) if match else None\n",
        "\n",
        "    print(\"Extracting cycle years...\")\n",
        "    df_long['Cycle_Year'] = df_long['Cycle'].apply(extract_year)\n",
        "\n",
        "    print(\"Sorting by patient and year...\")\n",
        "    df_long = df_long.sort_values(['SEQN', 'Cycle_Year']).reset_index(drop=True)\n",
        "\n",
        "    print(\"Calculating observation numbers...\")\n",
        "    df_long['Observation_Number'] = df_long.groupby('SEQN').cumcount() + 1\n",
        "\n",
        "    print(\"Calculating years since first observation...\")\n",
        "    df_long['Years_Since_First'] = df_long.groupby('SEQN')['Cycle_Year'].transform(\n",
        "        lambda x: x - x.iloc[0]\n",
        "    )\n",
        "\n",
        "    print(\"Adding first/last observation flags...\")\n",
        "    df_long['Is_First_Observation'] = df_long['Observation_Number'] == 1\n",
        "    df_long['Is_Last_Observation'] = (\n",
        "        df_long.groupby('SEQN')['Observation_Number'].transform('max') ==\n",
        "        df_long['Observation_Number']\n",
        "    )\n",
        "\n",
        "    print(\"Counting total observations per patient...\")\n",
        "    df_long['Total_Observations'] = df_long.groupby('SEQN')['SEQN'].transform('count')\n",
        "\n",
        "    temporal_time = (datetime.now() - temporal_start).total_seconds()\n",
        "\n",
        "    print(f\"âœ… Added temporal features in {temporal_time:.1f}s\")\n",
        "    print()\n",
        "\n",
        "    # ==========================================================================\n",
        "    # STEP 5: SAVE OUTPUT\n",
        "    # ==========================================================================\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(\"ðŸ’¾ STEP 5: SAVING OUTPUT\")\n",
        "    print(\"=\" * 70)\n",
        "    print()\n",
        "\n",
        "    save_start = datetime.now()\n",
        "\n",
        "    print(f\"Saving to {output_file}...\")\n",
        "    df_long.to_csv(output_file, index=False)\n",
        "\n",
        "    save_time = (datetime.now() - save_start).total_seconds()\n",
        "\n",
        "    print(f\"âœ… Saved in {save_time:.1f}s\")\n",
        "    print()\n",
        "\n",
        "    # ==========================================================================\n",
        "    # STEP 6: SUMMARY STATISTICS\n",
        "    # ==========================================================================\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(\"ðŸ“Š SUMMARY STATISTICS\")\n",
        "    print(\"=\" * 70)\n",
        "    print()\n",
        "\n",
        "    print(f\"Total patient-cycle observations: {len(df_long):,}\")\n",
        "    print(f\"Unique patients: {df_long['SEQN'].nunique():,}\")\n",
        "    print(f\"Variables: {len(df_long.columns):,}\")\n",
        "    print()\n",
        "\n",
        "    print(\"Observations per cycle:\")\n",
        "    cycle_dist = df_long['Cycle'].value_counts().sort_index()\n",
        "    for cycle, count in cycle_dist.items():\n",
        "        print(f\"   â€¢ {cycle}: {count:,}\")\n",
        "    print()\n",
        "\n",
        "    print(\"Patients by number of observations:\")\n",
        "    obs_dist = df_long.groupby('SEQN')['Total_Observations'].first().value_counts().sort_index()\n",
        "    for num_obs, count in obs_dist.items():\n",
        "        print(f\"   â€¢ {num_obs} observation(s): {count:,} patients\")\n",
        "    print()\n",
        "\n",
        "    multi_obs = df_long[df_long['Total_Observations'] > 1]\n",
        "    if len(multi_obs) > 0:\n",
        "        avg_span = multi_obs.groupby('SEQN')['Years_Since_First'].max().mean()\n",
        "        print(f\"Longitudinal tracking:\")\n",
        "        print(f\"   â€¢ Patients with 2+ observations: {len(multi_obs['SEQN'].unique()):,}\")\n",
        "        print(f\"   â€¢ Average time span: {avg_span:.1f} years\")\n",
        "    print()\n",
        "\n",
        "    # ==========================================================================\n",
        "    # FINAL TIMING\n",
        "    # ==========================================================================\n",
        "\n",
        "    total_time = (datetime.now() - start_time).total_seconds()\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(\"â±ï¸  PERFORMANCE BREAKDOWN\")\n",
        "    print(\"=\" * 70)\n",
        "    print()\n",
        "    print(f\"Load data:           {load_time:6.1f}s ({load_time/total_time*100:4.1f}%)\")\n",
        "    print(f\"Reshape (vectorized):{reshape_time:6.1f}s ({reshape_time/total_time*100:4.1f}%)\")\n",
        "    print(f\"Temporal features:   {temporal_time:6.1f}s ({temporal_time/total_time*100:4.1f}%)\")\n",
        "    print(f\"Save to CSV:         {save_time:6.1f}s ({save_time/total_time*100:4.1f}%)\")\n",
        "    print(f\"{'â”€'*50}\")\n",
        "    print(f\"TOTAL:               {total_time:6.1f}s ({total_time/60:5.1f} min)\")\n",
        "    print()\n",
        "\n",
        "    speedup = 1800 / total_time  # Assuming old version took 30 min\n",
        "    print(f\"ðŸš€ Speedup vs old version: ~{speedup:.0f}x faster!\")\n",
        "    print()\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(\"âœ… RESHAPING COMPLETE!\")\n",
        "    print(\"=\" * 70)\n",
        "    print()\n",
        "    print(f\"ðŸ“ Output: {output_file}\")\n",
        "    print(\"ðŸŽ‰ Ready for analysis!\")\n",
        "\n",
        "    return df_long\n",
        "\n",
        "# =============================================================================\n",
        "# RUN IT!\n",
        "# =============================================================================\n",
        "\n",
        "print(\"ðŸš€ Starting ULTRA-FAST NHANES reshaping...\")\n",
        "print()\n",
        "print(\"ðŸ’¡ Key differences from old version:\")\n",
        "print(\"   â€¢ Uses pd.concat instead of nested loops\")\n",
        "print(\"   â€¢ Processes whole cycles at once (vectorized)\")\n",
        "print(\"   â€¢ No chunk-by-chunk iteration through patients\")\n",
        "print(\"   â€¢ Should be ~10x faster!\")\n",
        "print()\n",
        "print(\"â±ï¸  Expected completion: 3-5 minutes\")\n",
        "print()\n",
        "print(\"Let's go! ðŸƒâ€â™‚ï¸ðŸ’¨\")\n",
        "print()\n",
        "\n",
        "df_long = ultra_fast_reshape_nhanes(\n",
        "    input_file='nhanes_patient_flattened_enhanced.csv',\n",
        "    output_file='nhanes_longitudinal.csv'\n",
        ")\n",
        "\n",
        "print(\"\\nðŸ“‹ Preview of longitudinal data:\")\n",
        "print(df_long.head(10))\n",
        "print()\n",
        "print(f\"Shape: {df_long.shape[0]:,} rows Ã— {df_long.shape[1]} columns\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGK0ss3NTPbv",
        "outputId": "64f84984-ba3e-4e7d-a375-763de99d34f3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Starting ULTRA-FAST NHANES reshaping...\n",
            "\n",
            "ðŸ’¡ Key differences from old version:\n",
            "   â€¢ Uses pd.concat instead of nested loops\n",
            "   â€¢ Processes whole cycles at once (vectorized)\n",
            "   â€¢ No chunk-by-chunk iteration through patients\n",
            "   â€¢ Should be ~10x faster!\n",
            "\n",
            "â±ï¸  Expected completion: 3-5 minutes\n",
            "\n",
            "Let's go! ðŸƒâ€â™‚ï¸ðŸ’¨\n",
            "\n",
            "======================================================================\n",
            "âš¡ ULTRA-FAST NHANES RESHAPER\n",
            "======================================================================\n",
            "\n",
            "Using vectorized pandas operations (pd.melt)\n",
            "Expected time: 3-5 minutes\n",
            "\n",
            "======================================================================\n",
            "ðŸ“‚ STEP 1: ANALYZING COLUMN STRUCTURE\n",
            "======================================================================\n",
            "\n",
            "Loading column names...\n",
            "âœ… Found 1,971 columns\n",
            "\n",
            "ðŸ” Detecting cycles...\n",
            "âœ… Found 12 cycles\n",
            "    1. 08_2021-08_2023\n",
            "    2. 1999-2000\n",
            "    3. 2001-2002\n",
            "    4. 2003-2004\n",
            "    5. 2005-2006\n",
            "    6. 2007-2008\n",
            "    7. 2009-2010\n",
            "    8. 2011-2012\n",
            "    9. 2013-2014\n",
            "   10. 2015-2016\n",
            "   11. 2017-2018\n",
            "   12. 2017-March_2020\n",
            "\n",
            "ðŸ—ºï¸  Mapping columns to cycles...\n",
            "   â€¢ 08_2021-08_2023: 102 columns\n",
            "   â€¢ 1999-2000: 133 columns\n",
            "   â€¢ 2001-2002: 104 columns\n",
            "   â€¢ 2003-2004: 142 columns\n",
            "   â€¢ 2005-2006: 118 columns\n",
            "   â€¢ 2007-2008: 147 columns\n",
            "   â€¢ 2009-2010: 116 columns\n",
            "   â€¢ 2011-2012: 238 columns\n",
            "   â€¢ 2013-2014: 237 columns\n",
            "   â€¢ 2015-2016: 148 columns\n",
            "   â€¢ 2017-2018: 149 columns\n",
            "   â€¢ 2017-March_2020: 161 columns\n",
            "\n",
            "======================================================================\n",
            "ðŸ“‚ STEP 2: LOADING DATA\n",
            "======================================================================\n",
            "\n",
            "Loading full dataset (this may take 60-90 seconds)...\n",
            "âœ… Loaded 136,803 patients in 43.4s\n",
            "\n",
            "======================================================================\n",
            "âš¡ STEP 3: VECTORIZED RESHAPING\n",
            "======================================================================\n",
            "\n",
            "Using pd.melt() - this is FAST!\n",
            "\n",
            "Processing cycle 1/12: 08_2021-08_2023...\n",
            "   â€¢ 11,873 patients have data in this cycle\n",
            "   âœ… Reshaped to 11,873 rows\n",
            "Processing cycle 2/12: 1999-2000...\n",
            "   â€¢ 9,943 patients have data in this cycle\n",
            "   âœ… Reshaped to 9,943 rows\n",
            "Processing cycle 3/12: 2001-2002...\n",
            "   â€¢ 10,468 patients have data in this cycle\n",
            "   âœ… Reshaped to 10,468 rows\n",
            "Processing cycle 4/12: 2003-2004...\n",
            "   â€¢ 10,109 patients have data in this cycle\n",
            "   âœ… Reshaped to 10,109 rows\n",
            "Processing cycle 5/12: 2005-2006...\n",
            "   â€¢ 10,332 patients have data in this cycle\n",
            "   âœ… Reshaped to 10,332 rows\n",
            "Processing cycle 6/12: 2007-2008...\n",
            "   â€¢ 10,121 patients have data in this cycle\n",
            "   âœ… Reshaped to 10,121 rows\n",
            "Processing cycle 7/12: 2009-2010...\n",
            "   â€¢ 10,527 patients have data in this cycle\n",
            "   âœ… Reshaped to 10,527 rows\n",
            "Processing cycle 8/12: 2011-2012...\n",
            "   â€¢ 9,746 patients have data in this cycle\n",
            "   âœ… Reshaped to 9,746 rows\n",
            "Processing cycle 9/12: 2013-2014...\n",
            "   â€¢ 10,161 patients have data in this cycle\n",
            "   âœ… Reshaped to 10,161 rows\n",
            "Processing cycle 10/12: 2015-2016...\n",
            "   â€¢ 9,954 patients have data in this cycle\n",
            "   âœ… Reshaped to 9,954 rows\n",
            "Processing cycle 11/12: 2017-2018...\n",
            "   â€¢ 9,235 patients have data in this cycle\n",
            "   âœ… Reshaped to 9,235 rows\n",
            "Processing cycle 12/12: 2017-March_2020...\n",
            "   â€¢ 23,197 patients have data in this cycle\n",
            "   âœ… Reshaped to 23,197 rows\n",
            "\n",
            "Combining all cycles...\n",
            "âœ… Created 135,666 patient-cycle observations in 2.1s\n",
            "\n",
            "======================================================================\n",
            "â° STEP 4: ADDING TEMPORAL FEATURES\n",
            "======================================================================\n",
            "\n",
            "Extracting cycle years...\n",
            "Sorting by patient and year...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2373243102.py:205: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_long['Cycle_Year'] = df_long['Cycle'].apply(extract_year)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating observation numbers...\n",
            "Calculating years since first observation...\n",
            "Adding first/last observation flags...\n",
            "Counting total observations per patient...\n",
            "âœ… Added temporal features in 21.4s\n",
            "\n",
            "======================================================================\n",
            "ðŸ’¾ STEP 5: SAVING OUTPUT\n",
            "======================================================================\n",
            "\n",
            "Saving to nhanes_longitudinal.csv...\n",
            "âœ… Saved in 23.0s\n",
            "\n",
            "======================================================================\n",
            "ðŸ“Š SUMMARY STATISTICS\n",
            "======================================================================\n",
            "\n",
            "Total patient-cycle observations: 135,666\n",
            "Unique patients: 126,769\n",
            "Variables: 410\n",
            "\n",
            "Observations per cycle:\n",
            "   â€¢ 08_2021-08_2023: 11,873\n",
            "   â€¢ 1999-2000: 9,943\n",
            "   â€¢ 2001-2002: 10,468\n",
            "   â€¢ 2003-2004: 10,109\n",
            "   â€¢ 2005-2006: 10,332\n",
            "   â€¢ 2007-2008: 10,121\n",
            "   â€¢ 2009-2010: 10,527\n",
            "   â€¢ 2011-2012: 9,746\n",
            "   â€¢ 2013-2014: 10,161\n",
            "   â€¢ 2015-2016: 9,954\n",
            "   â€¢ 2017-2018: 9,235\n",
            "   â€¢ 2017-March_2020: 23,197\n",
            "\n",
            "Patients by number of observations:\n",
            "   â€¢ 1 observation(s): 117,872 patients\n",
            "   â€¢ 2 observation(s): 8,897 patients\n",
            "\n",
            "Longitudinal tracking:\n",
            "   â€¢ Patients with 2+ observations: 8,897\n",
            "   â€¢ Average time span: 0.0 years\n",
            "\n",
            "======================================================================\n",
            "â±ï¸  PERFORMANCE BREAKDOWN\n",
            "======================================================================\n",
            "\n",
            "Load data:             43.4s (48.1%)\n",
            "Reshape (vectorized):   2.1s ( 2.3%)\n",
            "Temporal features:     21.4s (23.7%)\n",
            "Save to CSV:           23.0s (25.5%)\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "TOTAL:                 90.1s (  1.5 min)\n",
            "\n",
            "ðŸš€ Speedup vs old version: ~20x faster!\n",
            "\n",
            "======================================================================\n",
            "âœ… RESHAPING COMPLETE!\n",
            "======================================================================\n",
            "\n",
            "ðŸ“ Output: nhanes_longitudinal.csv\n",
            "ðŸŽ‰ Ready for analysis!\n",
            "\n",
            "ðŸ“‹ Preview of longitudinal data:\n",
            "   SEQN       WTSAF2YR  LBXTLG  LBDTRSI  LBDLDL  LBDLDLSI  LBDLDLM  LBDLDMSI  \\\n",
            "0     1            NaN     NaN      NaN     NaN       NaN      NaN       NaN   \n",
            "1     2   60586.147294     NaN     1.45   136.0      3.52      NaN       NaN   \n",
            "2     3            NaN     NaN     2.28    58.0      1.50      NaN       NaN   \n",
            "3     4            NaN     NaN      NaN     NaN       NaN      NaN       NaN   \n",
            "4     5  234895.205650     NaN     3.92   168.0      4.34      NaN       NaN   \n",
            "5     6            NaN     NaN      NaN     NaN       NaN      NaN       NaN   \n",
            "6     7   57661.621988     NaN     0.70   127.0      3.28      NaN       NaN   \n",
            "7     8   76026.438279     NaN     0.37    88.0      2.28      NaN       NaN   \n",
            "8     9            NaN     NaN     0.63    79.0      2.04      NaN       NaN   \n",
            "9    10   60202.416895     NaN     0.51    80.0      2.07      NaN       NaN   \n",
            "\n",
            "   LBDLDLN  LBDLDNSI  ...  BMDSADCM  DIQ175X  BPAOMNTS  WTFOL2YR  Cycle_Year  \\\n",
            "0      NaN       NaN  ...       NaN      NaN       NaN       NaN        1999   \n",
            "1      NaN       NaN  ...       NaN      NaN       NaN       NaN        1999   \n",
            "2      NaN       NaN  ...       NaN      NaN       NaN       NaN        1999   \n",
            "3      NaN       NaN  ...       NaN      NaN       NaN       NaN        1999   \n",
            "4      NaN       NaN  ...       NaN      NaN       NaN       NaN        1999   \n",
            "5      NaN       NaN  ...       NaN      NaN       NaN       NaN        1999   \n",
            "6      NaN       NaN  ...       NaN      NaN       NaN       NaN        1999   \n",
            "7      NaN       NaN  ...       NaN      NaN       NaN       NaN        1999   \n",
            "8      NaN       NaN  ...       NaN      NaN       NaN       NaN        1999   \n",
            "9      NaN       NaN  ...       NaN      NaN       NaN       NaN        1999   \n",
            "\n",
            "   Observation_Number  Years_Since_First  Is_First_Observation  \\\n",
            "0                   1                  0                  True   \n",
            "1                   1                  0                  True   \n",
            "2                   1                  0                  True   \n",
            "3                   1                  0                  True   \n",
            "4                   1                  0                  True   \n",
            "5                   1                  0                  True   \n",
            "6                   1                  0                  True   \n",
            "7                   1                  0                  True   \n",
            "8                   1                  0                  True   \n",
            "9                   1                  0                  True   \n",
            "\n",
            "   Is_Last_Observation  Total_Observations  \n",
            "0                 True                   1  \n",
            "1                 True                   1  \n",
            "2                 True                   1  \n",
            "3                 True                   1  \n",
            "4                 True                   1  \n",
            "5                 True                   1  \n",
            "6                 True                   1  \n",
            "7                 True                   1  \n",
            "8                 True                   1  \n",
            "9                 True                   1  \n",
            "\n",
            "[10 rows x 410 columns]\n",
            "\n",
            "Shape: 135,666 rows Ã— 410 columns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "FIX LONGITUDINAL DATA & PREPARE FOR MODELING\n",
        "=============================================\n",
        "This script:\n",
        "1. Adds demographic variables (age, gender, race) to longitudinal data\n",
        "2. Fixes the time span calculation bug\n",
        "3. Creates a clean modeling-ready dataset\n",
        "\n",
        "Author: Claude + User\n",
        "Date: 2025\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ðŸ”§ FIXING LONGITUDINAL DATA & PREPARING FOR MODELING\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 1: LOAD DATA\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 1: LOADING DATA\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "print(\"Loading wide format (for demographics)...\")\n",
        "df_wide = pd.read_csv('nhanes_patient_flattened_enhanced.csv', low_memory=False)\n",
        "print(f\"âœ… Wide format: {len(df_wide):,} rows\")\n",
        "\n",
        "print(\"Loading longitudinal format...\")\n",
        "df_long = pd.read_csv('nhanes_longitudinal.csv', low_memory=False)\n",
        "print(f\"âœ… Longitudinal format: {len(df_long):,} observations\")\n",
        "print()\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 2: EXTRACT DEMOGRAPHICS FROM WIDE FORMAT\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 2: EXTRACTING DEMOGRAPHIC VARIABLES\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "# Find demographic columns (they don't have cycle names)\n",
        "print(\"Searching for demographic variables in wide format...\")\n",
        "\n",
        "demo_patterns = {\n",
        "    'Age': 'RIDAGEYR',\n",
        "    'Age_Months': 'RIDAGEMN',\n",
        "    'Gender': 'RIAGENDR',\n",
        "    'Race': 'RIDRETH',\n",
        "    'Education': 'DMDEDUC',\n",
        "    'Marital_Status': 'DMDMARTL',\n",
        "    'Income': 'INDFMPIR',\n",
        "}\n",
        "\n",
        "demo_cols_found = {}\n",
        "demo_data = {'SEQN': df_wide['SEQN']}\n",
        "\n",
        "for var_name, pattern in demo_patterns.items():\n",
        "    # Find columns matching pattern\n",
        "    matching = [col for col in df_wide.columns if pattern in col.upper() and 'DEMO' in col.upper()]\n",
        "\n",
        "    if matching:\n",
        "        # Use first match\n",
        "        col = matching[0]\n",
        "        demo_cols_found[var_name] = col\n",
        "        demo_data[var_name] = df_wide[col]\n",
        "\n",
        "        # Show sample statistics\n",
        "        non_null = df_wide[col].notna().sum()\n",
        "        if pd.api.types.is_numeric_dtype(df_wide[col]):\n",
        "            print(f\"âœ… {var_name:20} ({col})\")\n",
        "            print(f\"   â€¢ Values: {non_null:,} ({non_null/len(df_wide)*100:.1f}% complete)\")\n",
        "            print(f\"   â€¢ Range: {df_wide[col].min():.0f} to {df_wide[col].max():.0f}\")\n",
        "        else:\n",
        "            print(f\"âœ… {var_name:20} ({col})\")\n",
        "            print(f\"   â€¢ Values: {non_null:,} ({non_null/len(df_wide)*100:.1f}% complete)\")\n",
        "    else:\n",
        "        print(f\"âš ï¸  {var_name:20} - Not found\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Create demographics dataframe\n",
        "df_demo = pd.DataFrame(demo_data)\n",
        "\n",
        "# Handle duplicate SEQN by keeping first occurrence\n",
        "df_demo = df_demo.drop_duplicates(subset='SEQN', keep='first')\n",
        "print(f\"ðŸ“Š Extracted demographics for {len(df_demo):,} unique patients\")\n",
        "print()\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 3: MERGE DEMOGRAPHICS INTO LONGITUDINAL DATA\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 3: MERGING DEMOGRAPHICS\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "print(\"Merging demographics into longitudinal data...\")\n",
        "\n",
        "# Merge on SEQN\n",
        "df_enhanced = df_long.merge(df_demo, on='SEQN', how='left')\n",
        "\n",
        "print(f\"âœ… Enhanced dataset: {len(df_enhanced):,} observations\")\n",
        "print()\n",
        "\n",
        "# Check merge success\n",
        "print(\"Merge statistics:\")\n",
        "for var_name in demo_cols_found.keys():\n",
        "    if var_name in df_enhanced.columns:\n",
        "        non_null = df_enhanced[var_name].notna().sum()\n",
        "        print(f\"   â€¢ {var_name:20} {non_null:,} values ({non_null/len(df_enhanced)*100:.1f}% complete)\")\n",
        "print()\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 4: FIX TIME SPAN CALCULATION\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 4: FIXING TIME SPAN CALCULATION\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "print(\"Current time span calculation has a bug for special cycles:\")\n",
        "print(\"   â€¢ '2017-March_2020' extracts as 2017 (should be ~2019)\")\n",
        "print(\"   â€¢ '08_2021-08_2023' extracts as 2021 (should be ~2022)\")\n",
        "print()\n",
        "\n",
        "def extract_year_fixed(cycle):\n",
        "    \"\"\"\n",
        "    Extract year from cycle, handling special formats correctly.\n",
        "\n",
        "    For ranges, use the START year for consistent ordering.\n",
        "    \"\"\"\n",
        "    cycle_str = str(cycle)\n",
        "\n",
        "    # Special case: 2017-March_2020\n",
        "    if '2017-March_2020' in cycle_str:\n",
        "        return 2017  # Keep as 2017 (start year)\n",
        "\n",
        "    # Special case: 08_2021-08_2023\n",
        "    elif '08_2021-08_2023' in cycle_str or '2021-08_2023' in cycle_str:\n",
        "        return 2021  # Keep as 2021 (start year)\n",
        "\n",
        "    # Standard format: YYYY-YYYY (use first year)\n",
        "    elif re.match(r'(\\d{4})-(\\d{4})', cycle_str):\n",
        "        match = re.match(r'(\\d{4})-(\\d{4})', cycle_str)\n",
        "        return int(match.group(1))\n",
        "\n",
        "    # Fallback: find first 4-digit year\n",
        "    else:\n",
        "        match = re.search(r'(\\d{4})', cycle_str)\n",
        "        if match:\n",
        "            return int(match.group(1))\n",
        "        return None\n",
        "\n",
        "# Apply fixed year extraction\n",
        "print(\"Recalculating Cycle_Year...\")\n",
        "df_enhanced['Cycle_Year_Fixed'] = df_enhanced['Cycle'].apply(extract_year_fixed)\n",
        "\n",
        "# Recalculate years since first\n",
        "print(\"Recalculating Years_Since_First...\")\n",
        "df_enhanced = df_enhanced.sort_values(['SEQN', 'Cycle_Year_Fixed']).reset_index(drop=True)\n",
        "\n",
        "df_enhanced['Years_Since_First_Fixed'] = df_enhanced.groupby('SEQN')['Cycle_Year_Fixed'].transform(\n",
        "    lambda x: x - x.iloc[0]\n",
        ")\n",
        "\n",
        "# Show comparison\n",
        "print()\n",
        "print(\"Comparison (old vs fixed):\")\n",
        "print()\n",
        "sample_multi = df_enhanced[df_enhanced['Total_Observations'] > 1].groupby('SEQN').head(2)\n",
        "if len(sample_multi) > 0:\n",
        "    print(sample_multi[['SEQN', 'Cycle', 'Cycle_Year', 'Years_Since_First',\n",
        "                        'Cycle_Year_Fixed', 'Years_Since_First_Fixed']].head(10).to_string())\n",
        "    print()\n",
        "\n",
        "# Check if fix worked\n",
        "max_span_old = df_enhanced.groupby('SEQN')['Years_Since_First'].max().max()\n",
        "max_span_new = df_enhanced.groupby('SEQN')['Years_Since_First_Fixed'].max().max()\n",
        "\n",
        "print(f\"Max time span (old): {max_span_old} years\")\n",
        "print(f\"Max time span (new): {max_span_new} years\")\n",
        "print()\n",
        "\n",
        "if max_span_new > max_span_old:\n",
        "    print(\"âœ… Fix successful! Time spans now calculated correctly.\")\n",
        "else:\n",
        "    print(\"âš ï¸  Time spans unchanged - may need further investigation\")\n",
        "print()\n",
        "\n",
        "# Replace old columns with fixed versions\n",
        "df_enhanced['Cycle_Year'] = df_enhanced['Cycle_Year_Fixed']\n",
        "df_enhanced['Years_Since_First'] = df_enhanced['Years_Since_First_Fixed']\n",
        "df_enhanced = df_enhanced.drop(['Cycle_Year_Fixed', 'Years_Since_First_Fixed'], axis=1)\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 5: CREATE DIABETES TARGET VARIABLE\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 5: CREATING DIABETES TARGET VARIABLE\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "print(\"Defining diabetes using multiple criteria...\")\n",
        "\n",
        "# Initialize diabetes flags\n",
        "df_enhanced['Diabetes_SelfReport'] = False\n",
        "df_enhanced['Diabetes_Glucose'] = False\n",
        "df_enhanced['Diabetes_HbA1c'] = False\n",
        "\n",
        "# Method 1: Self-report (DIQ010 = 1 means \"Yes\")\n",
        "if 'DIQ010' in df_enhanced.columns:\n",
        "    df_enhanced['Diabetes_SelfReport'] = (df_enhanced['DIQ010'] == 1)\n",
        "    count_self = df_enhanced['Diabetes_SelfReport'].sum()\n",
        "    print(f\"âœ… Self-report diabetes: {count_self:,} cases ({count_self/len(df_enhanced)*100:.1f}%)\")\n",
        "\n",
        "# Method 2: Fasting glucose >= 126 mg/dL\n",
        "if 'LBXGLU' in df_enhanced.columns:\n",
        "    df_enhanced['Diabetes_Glucose'] = (df_enhanced['LBXGLU'] >= 126)\n",
        "    count_gluc = df_enhanced['Diabetes_Glucose'].sum()\n",
        "    print(f\"âœ… Glucose â‰¥126: {count_gluc:,} cases ({count_gluc/len(df_enhanced)*100:.1f}%)\")\n",
        "\n",
        "# Method 3: HbA1c >= 6.5%\n",
        "if 'LBXGH' in df_enhanced.columns:\n",
        "    df_enhanced['Diabetes_HbA1c'] = (df_enhanced['LBXGH'] >= 6.5)\n",
        "    count_hba1c = df_enhanced['Diabetes_HbA1c'].sum()\n",
        "    print(f\"âœ… HbA1c â‰¥6.5: {count_hba1c:,} cases ({count_hba1c/len(df_enhanced)*100:.1f}%)\")\n",
        "\n",
        "# Combined: Has diabetes if ANY criterion is met\n",
        "df_enhanced['Has_Diabetes'] = (\n",
        "    df_enhanced['Diabetes_SelfReport'] |\n",
        "    df_enhanced['Diabetes_Glucose'] |\n",
        "    df_enhanced['Diabetes_HbA1c']\n",
        ")\n",
        "\n",
        "diabetes_total = df_enhanced['Has_Diabetes'].sum()\n",
        "print()\n",
        "print(f\"ðŸ“Š Combined diabetes definition: {diabetes_total:,} cases ({diabetes_total/len(df_enhanced)*100:.1f}%)\")\n",
        "print()\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 6: CREATE MODELING-READY DATASET\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 6: CREATING MODELING-READY DATASET\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "print(\"Applying filters for clean modeling dataset...\")\n",
        "print()\n",
        "\n",
        "# Start with all data\n",
        "df_model = df_enhanced.copy()\n",
        "print(f\"Starting with: {len(df_model):,} observations\")\n",
        "\n",
        "# Filter 1: First observations only (cross-sectional)\n",
        "df_model = df_model[df_model['Is_First_Observation'] == True]\n",
        "print(f\"After keeping first observations only: {len(df_model):,}\")\n",
        "\n",
        "# Filter 2: Must have key variables\n",
        "print()\n",
        "print(\"Checking key variable availability...\")\n",
        "\n",
        "key_vars = ['LBXGLU', 'LBXGH', 'BMXBMI']\n",
        "available_vars = [var for var in key_vars if var in df_model.columns]\n",
        "\n",
        "print(f\"Available key variables: {available_vars}\")\n",
        "print()\n",
        "\n",
        "# Filter 3: Must have at least ONE key diabetes indicator\n",
        "df_model['Has_Key_Data'] = (\n",
        "    df_model['LBXGLU'].notna() |\n",
        "    df_model['LBXGH'].notna() |\n",
        "    df_model['BMXBMI'].notna()\n",
        ")\n",
        "\n",
        "df_model = df_model[df_model['Has_Key_Data']]\n",
        "print(f\"After requiring at least one key variable: {len(df_model):,}\")\n",
        "\n",
        "# Filter 4: Must have age (critical demographic)\n",
        "if 'Age' in df_model.columns:\n",
        "    df_model = df_model[df_model['Age'].notna()]\n",
        "    print(f\"After requiring Age: {len(df_model):,}\")\n",
        "\n",
        "# Filter 5: Remove extreme outliers\n",
        "print()\n",
        "print(\"Removing extreme outliers...\")\n",
        "\n",
        "# BMI outliers (< 12 or > 70 are likely errors)\n",
        "if 'BMXBMI' in df_model.columns:\n",
        "    before = len(df_model)\n",
        "    df_model = df_model[(df_model['BMXBMI'].isna()) |\n",
        "                        ((df_model['BMXBMI'] >= 12) & (df_model['BMXBMI'] <= 70))]\n",
        "    removed = before - len(df_model)\n",
        "    print(f\"   â€¢ Removed {removed} extreme BMI outliers\")\n",
        "\n",
        "# Glucose outliers (< 30 or > 600 are likely errors)\n",
        "if 'LBXGLU' in df_model.columns:\n",
        "    before = len(df_model)\n",
        "    df_model = df_model[(df_model['LBXGLU'].isna()) |\n",
        "                        ((df_model['LBXGLU'] >= 30) & (df_model['LBXGLU'] <= 600))]\n",
        "    removed = before - len(df_model)\n",
        "    print(f\"   â€¢ Removed {removed} extreme glucose outliers\")\n",
        "\n",
        "# HbA1c outliers (< 3 or > 18 are likely errors)\n",
        "if 'LBXGH' in df_model.columns:\n",
        "    before = len(df_model)\n",
        "    df_model = df_model[(df_model['LBXGH'].isna()) |\n",
        "                        ((df_model['LBXGH'] >= 3) & (df_model['LBXGH'] <= 18))]\n",
        "    removed = before - len(df_model)\n",
        "    print(f\"   â€¢ Removed {removed} extreme HbA1c outliers\")\n",
        "\n",
        "# Age outliers (< 18 for adult diabetes prediction)\n",
        "if 'Age' in df_model.columns:\n",
        "    before = len(df_model)\n",
        "    df_model = df_model[df_model['Age'] >= 18]\n",
        "    removed = before - len(df_model)\n",
        "    print(f\"   â€¢ Removed {removed} under-18 (adults only)\")\n",
        "\n",
        "print()\n",
        "print(f\"âœ… Final modeling dataset: {len(df_model):,} observations\")\n",
        "print()\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 7: CREATE ENGINEERED FEATURES\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 7: FEATURE ENGINEERING\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "# BMI Categories\n",
        "if 'BMXBMI' in df_model.columns:\n",
        "    print(\"Creating BMI categories...\")\n",
        "    df_model['BMI_Category'] = pd.cut(\n",
        "        df_model['BMXBMI'],\n",
        "        bins=[0, 18.5, 25, 30, 100],\n",
        "        labels=['Underweight', 'Normal', 'Overweight', 'Obese']\n",
        "    )\n",
        "    print(\"   â€¢ Categories: Underweight, Normal, Overweight, Obese\")\n",
        "    print(df_model['BMI_Category'].value_counts().to_string())\n",
        "    print()\n",
        "\n",
        "# Age Groups\n",
        "if 'Age' in df_model.columns:\n",
        "    print(\"Creating age groups...\")\n",
        "    df_model['Age_Group'] = pd.cut(\n",
        "        df_model['Age'],\n",
        "        bins=[0, 30, 40, 50, 60, 70, 120],\n",
        "        labels=['18-29', '30-39', '40-49', '50-59', '60-69', '70+']\n",
        "    )\n",
        "    print(\"   â€¢ Groups: 18-29, 30-39, 40-49, 50-59, 60-69, 70+\")\n",
        "    print(df_model['Age_Group'].value_counts().to_string())\n",
        "    print()\n",
        "\n",
        "# Glucose Categories\n",
        "if 'LBXGLU' in df_model.columns:\n",
        "    print(\"Creating glucose categories...\")\n",
        "    df_model['Glucose_Category'] = pd.cut(\n",
        "        df_model['LBXGLU'],\n",
        "        bins=[0, 100, 126, 1000],\n",
        "        labels=['Normal', 'Prediabetes', 'Diabetes']\n",
        "    )\n",
        "    print(\"   â€¢ Categories: Normal (<100), Prediabetes (100-125), Diabetes (â‰¥126)\")\n",
        "    print(df_model['Glucose_Category'].value_counts().to_string())\n",
        "    print()\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 8: SAVE OUTPUTS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 8: SAVING OUTPUTS\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "# Save enhanced longitudinal data (all observations, with demographics)\n",
        "print(\"Saving enhanced longitudinal data...\")\n",
        "df_enhanced.to_csv('nhanes_longitudinal_enhanced.csv', index=False)\n",
        "print(f\"âœ… Saved: nhanes_longitudinal_enhanced.csv\")\n",
        "print(f\"   â€¢ {len(df_enhanced):,} observations\")\n",
        "print(f\"   â€¢ {len(df_enhanced.columns)} variables\")\n",
        "print()\n",
        "\n",
        "# Save modeling-ready dataset (first observations, cleaned)\n",
        "print(\"Saving modeling-ready dataset...\")\n",
        "df_model.to_csv('nhanes_modeling_ready.csv', index=False)\n",
        "print(f\"âœ… Saved: nhanes_modeling_ready.csv\")\n",
        "print(f\"   â€¢ {len(df_model):,} observations\")\n",
        "print(f\"   â€¢ {len(df_model.columns)} variables\")\n",
        "print()\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 9: GENERATE SUMMARY REPORT\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ðŸ“Š SUMMARY STATISTICS\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "print(\"ENHANCED LONGITUDINAL DATA:\")\n",
        "print(f\"   â€¢ Total observations: {len(df_enhanced):,}\")\n",
        "print(f\"   â€¢ Unique patients: {df_enhanced['SEQN'].nunique():,}\")\n",
        "print(f\"   â€¢ Variables: {len(df_enhanced.columns)}\")\n",
        "print()\n",
        "\n",
        "print(\"MODELING-READY DATA:\")\n",
        "print(f\"   â€¢ Observations: {len(df_model):,}\")\n",
        "print(f\"   â€¢ Unique patients: {df_model['SEQN'].nunique():,}\")\n",
        "print(f\"   â€¢ Variables: {len(df_model.columns)}\")\n",
        "print()\n",
        "\n",
        "if 'Age' in df_model.columns:\n",
        "    print(f\"   â€¢ Age range: {df_model['Age'].min():.0f} to {df_model['Age'].max():.0f}\")\n",
        "    print(f\"   â€¢ Mean age: {df_model['Age'].mean():.1f} years\")\n",
        "\n",
        "if 'Gender' in df_model.columns:\n",
        "    print()\n",
        "    print(\"   Gender distribution:\")\n",
        "    gender_dist = df_model['Gender'].value_counts()\n",
        "    for gender, count in gender_dist.items():\n",
        "        print(f\"      â€¢ Gender {gender}: {count:,} ({count/len(df_model)*100:.1f}%)\")\n",
        "\n",
        "print()\n",
        "print(\"   Diabetes prevalence:\")\n",
        "diabetes_prev = df_model['Has_Diabetes'].sum()\n",
        "print(f\"      â€¢ {diabetes_prev:,} cases ({diabetes_prev/len(df_model)*100:.1f}%)\")\n",
        "\n",
        "print()\n",
        "print(\"   Data completeness (key variables):\")\n",
        "for var in ['LBXGLU', 'LBXGH', 'BMXBMI', 'Age', 'Gender']:\n",
        "    if var in df_model.columns:\n",
        "        complete = df_model[var].notna().sum()\n",
        "        pct = complete / len(df_model) * 100\n",
        "        print(f\"      â€¢ {var:10} {complete:6,} ({pct:5.1f}%)\")\n",
        "\n",
        "print()\n",
        "\n",
        "# =============================================================================\n",
        "# FINAL SUMMARY\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"âœ… ALL FIXES APPLIED & DATA PREPARED!\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "print(\"What was fixed:\")\n",
        "print(\"   âœ… Added demographics (age, gender, race, etc.)\")\n",
        "print(\"   âœ… Fixed time span calculation\")\n",
        "print(\"   âœ… Created diabetes target variable\")\n",
        "print(\"   âœ… Removed outliers and bad data\")\n",
        "print(\"   âœ… Created feature engineering\")\n",
        "print()\n",
        "\n",
        "print(\"Output files:\")\n",
        "print(\"   1. nhanes_longitudinal_enhanced.csv\")\n",
        "print(\"      â†’ All observations with demographics\")\n",
        "print(\"      â†’ Use for longitudinal analysis\")\n",
        "print()\n",
        "print(\"   2. nhanes_modeling_ready.csv\")\n",
        "print(\"      â†’ Clean, first observations only\")\n",
        "print(\"      â†’ Ready for machine learning!\")\n",
        "print()\n",
        "\n",
        "print(\"Next steps:\")\n",
        "print(\"   1. Review data quality\")\n",
        "print(\"   2. Handle remaining missing data (imputation)\")\n",
        "print(\"   3. Split into train/test sets\")\n",
        "print(\"   4. Build baseline model\")\n",
        "print(\"   5. Feature selection\")\n",
        "print(\"   6. Model tuning\")\n",
        "print()\n",
        "\n",
        "print(\"ðŸŽ‰ Ready to build your diabetes prediction model!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gd4EZ-oyU-RQ",
        "outputId": "d8bc5368-cb03-4860-bc34-8abec9d8b3d4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ðŸ”§ FIXING LONGITUDINAL DATA & PREPARING FOR MODELING\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "STEP 1: LOADING DATA\n",
            "================================================================================\n",
            "\n",
            "Loading wide format (for demographics)...\n",
            "âœ… Wide format: 136,803 rows\n",
            "Loading longitudinal format...\n",
            "âœ… Longitudinal format: 135,666 observations\n",
            "\n",
            "================================================================================\n",
            "STEP 2: EXTRACTING DEMOGRAPHIC VARIABLES\n",
            "================================================================================\n",
            "\n",
            "Searching for demographic variables in wide format...\n",
            "âœ… Age                  (DEMO_RIDAGEYR)\n",
            "   â€¢ Values: 122,503 (89.5% complete)\n",
            "   â€¢ Range: 0 to 85\n",
            "âœ… Age_Months           (DEMO_RIDAGEMN)\n",
            "   â€¢ Values: 64,059 (46.8% complete)\n",
            "   â€¢ Range: 0 to 1019\n",
            "âœ… Gender               (DEMO_RIAGENDR)\n",
            "   â€¢ Values: 122,503 (89.5% complete)\n",
            "   â€¢ Range: 1 to 2\n",
            "âœ… Race                 (DEMO_RIDRETH1)\n",
            "   â€¢ Values: 122,503 (89.5% complete)\n",
            "   â€¢ Range: 1 to 5\n",
            "âœ… Education            (DEMO_DMDEDUC3)\n",
            "   â€¢ Values: 31,781 (23.2% complete)\n",
            "   â€¢ Range: 0 to 99\n",
            "âœ… Marital_Status       (DEMO_DMDMARTL)\n",
            "   â€¢ Values: 67,132 (49.1% complete)\n",
            "   â€¢ Range: 1 to 99\n",
            "âœ… Income               (DEMO_INDFMPIR)\n",
            "   â€¢ Values: 110,035 (80.4% complete)\n",
            "   â€¢ Range: 0 to 5\n",
            "\n",
            "ðŸ“Š Extracted demographics for 127,549 unique patients\n",
            "\n",
            "================================================================================\n",
            "STEP 3: MERGING DEMOGRAPHICS\n",
            "================================================================================\n",
            "\n",
            "Merging demographics into longitudinal data...\n",
            "âœ… Enhanced dataset: 135,666 observations\n",
            "\n",
            "Merge statistics:\n",
            "   â€¢ Age                  121,366 values (89.5% complete)\n",
            "   â€¢ Age_Months           62,926 values (46.4% complete)\n",
            "   â€¢ Gender               121,366 values (89.5% complete)\n",
            "   â€¢ Race                 121,366 values (89.5% complete)\n",
            "   â€¢ Education            31,781 values (23.4% complete)\n",
            "   â€¢ Marital_Status       67,132 values (49.5% complete)\n",
            "   â€¢ Income               109,028 values (80.4% complete)\n",
            "\n",
            "================================================================================\n",
            "STEP 4: FIXING TIME SPAN CALCULATION\n",
            "================================================================================\n",
            "\n",
            "Current time span calculation has a bug for special cycles:\n",
            "   â€¢ '2017-March_2020' extracts as 2017 (should be ~2019)\n",
            "   â€¢ '08_2021-08_2023' extracts as 2021 (should be ~2022)\n",
            "\n",
            "Recalculating Cycle_Year...\n",
            "Recalculating Years_Since_First...\n",
            "\n",
            "Comparison (old vs fixed):\n",
            "\n",
            "        SEQN            Cycle  Cycle_Year  Years_Since_First  Cycle_Year_Fixed  Years_Since_First_Fixed\n",
            "91361  93703        2017-2018        2017                  0              2017                        0\n",
            "91362  93703  2017-March_2020        2017                  0              2017                        0\n",
            "91363  93704        2017-2018        2017                  0              2017                        0\n",
            "91364  93704  2017-March_2020        2017                  0              2017                        0\n",
            "91365  93705        2017-2018        2017                  0              2017                        0\n",
            "91366  93705  2017-March_2020        2017                  0              2017                        0\n",
            "91367  93706        2017-2018        2017                  0              2017                        0\n",
            "91368  93706  2017-March_2020        2017                  0              2017                        0\n",
            "91369  93707        2017-2018        2017                  0              2017                        0\n",
            "91370  93707  2017-March_2020        2017                  0              2017                        0\n",
            "\n",
            "Max time span (old): 0 years\n",
            "Max time span (new): 0 years\n",
            "\n",
            "âš ï¸  Time spans unchanged - may need further investigation\n",
            "\n",
            "================================================================================\n",
            "STEP 5: CREATING DIABETES TARGET VARIABLE\n",
            "================================================================================\n",
            "\n",
            "Defining diabetes using multiple criteria...\n",
            "âœ… Self-report diabetes: 8,781 cases (6.5%)\n",
            "âœ… Glucose â‰¥126: 3,910 cases (2.9%)\n",
            "âœ… HbA1c â‰¥6.5: 7,481 cases (5.5%)\n",
            "\n",
            "ðŸ“Š Combined diabetes definition: 12,450 cases (9.2%)\n",
            "\n",
            "================================================================================\n",
            "STEP 6: CREATING MODELING-READY DATASET\n",
            "================================================================================\n",
            "\n",
            "Applying filters for clean modeling dataset...\n",
            "\n",
            "Starting with: 135,666 observations\n",
            "After keeping first observations only: 126,769\n",
            "\n",
            "Checking key variable availability...\n",
            "Available key variables: ['LBXGLU', 'LBXGH', 'BMXBMI']\n",
            "\n",
            "After requiring at least one key variable: 108,395\n",
            "After requiring Age: 95,101\n",
            "\n",
            "Removing extreme outliers...\n",
            "   â€¢ Removed 28 extreme BMI outliers\n",
            "   â€¢ Removed 2 extreme glucose outliers\n",
            "   â€¢ Removed 3 extreme HbA1c outliers\n",
            "   â€¢ Removed 32830 under-18 (adults only)\n",
            "\n",
            "âœ… Final modeling dataset: 62,238 observations\n",
            "\n",
            "================================================================================\n",
            "STEP 7: FEATURE ENGINEERING\n",
            "================================================================================\n",
            "\n",
            "Creating BMI categories...\n",
            "   â€¢ Categories: Underweight, Normal, Overweight, Obese\n",
            "BMI_Category\n",
            "Obese          20297\n",
            "Overweight     18304\n",
            "Normal         16502\n",
            "Underweight     1083\n",
            "\n",
            "Creating age groups...\n",
            "   â€¢ Groups: 18-29, 30-39, 40-49, 50-59, 60-69, 70+\n",
            "Age_Group\n",
            "18-29    14891\n",
            "30-39     9732\n",
            "70+       9604\n",
            "60-69     9573\n",
            "40-49     9402\n",
            "50-59     9036\n",
            "\n",
            "Creating glucose categories...\n",
            "   â€¢ Categories: Normal (<100), Prediabetes (100-125), Diabetes (â‰¥126)\n",
            "Glucose_Category\n",
            "Normal         15876\n",
            "Prediabetes     9969\n",
            "Diabetes        3359\n",
            "\n",
            "================================================================================\n",
            "STEP 8: SAVING OUTPUTS\n",
            "================================================================================\n",
            "\n",
            "Saving enhanced longitudinal data...\n",
            "âœ… Saved: nhanes_longitudinal_enhanced.csv\n",
            "   â€¢ 135,666 observations\n",
            "   â€¢ 421 variables\n",
            "\n",
            "Saving modeling-ready dataset...\n",
            "âœ… Saved: nhanes_modeling_ready.csv\n",
            "   â€¢ 62,238 observations\n",
            "   â€¢ 425 variables\n",
            "\n",
            "================================================================================\n",
            "ðŸ“Š SUMMARY STATISTICS\n",
            "================================================================================\n",
            "\n",
            "ENHANCED LONGITUDINAL DATA:\n",
            "   â€¢ Total observations: 135,666\n",
            "   â€¢ Unique patients: 126,769\n",
            "   â€¢ Variables: 421\n",
            "\n",
            "MODELING-READY DATA:\n",
            "   â€¢ Observations: 62,238\n",
            "   â€¢ Unique patients: 62,238\n",
            "   â€¢ Variables: 425\n",
            "\n",
            "   â€¢ Age range: 18 to 85\n",
            "   â€¢ Mean age: 48.0 years\n",
            "\n",
            "   Gender distribution:\n",
            "      â€¢ Gender 2.0: 32,388 (52.0%)\n",
            "      â€¢ Gender 1.0: 29,850 (48.0%)\n",
            "\n",
            "   Diabetes prevalence:\n",
            "      â€¢ 9,527 cases (15.3%)\n",
            "\n",
            "   Data completeness (key variables):\n",
            "      â€¢ LBXGLU     29,204 ( 46.9%)\n",
            "      â€¢ LBXGH      59,385 ( 95.4%)\n",
            "      â€¢ BMXBMI     56,186 ( 90.3%)\n",
            "      â€¢ Age        62,238 (100.0%)\n",
            "      â€¢ Gender     62,238 (100.0%)\n",
            "\n",
            "================================================================================\n",
            "âœ… ALL FIXES APPLIED & DATA PREPARED!\n",
            "================================================================================\n",
            "\n",
            "What was fixed:\n",
            "   âœ… Added demographics (age, gender, race, etc.)\n",
            "   âœ… Fixed time span calculation\n",
            "   âœ… Created diabetes target variable\n",
            "   âœ… Removed outliers and bad data\n",
            "   âœ… Created feature engineering\n",
            "\n",
            "Output files:\n",
            "   1. nhanes_longitudinal_enhanced.csv\n",
            "      â†’ All observations with demographics\n",
            "      â†’ Use for longitudinal analysis\n",
            "\n",
            "   2. nhanes_modeling_ready.csv\n",
            "      â†’ Clean, first observations only\n",
            "      â†’ Ready for machine learning!\n",
            "\n",
            "Next steps:\n",
            "   1. Review data quality\n",
            "   2. Handle remaining missing data (imputation)\n",
            "   3. Split into train/test sets\n",
            "   4. Build baseline model\n",
            "   5. Feature selection\n",
            "   6. Model tuning\n",
            "\n",
            "ðŸŽ‰ Ready to build your diabetes prediction model!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "FINAL PREPROCESSING & DATA VISUALIZATION\n",
        "=========================================\n",
        "This script:\n",
        "1. Creates comprehensive visualizations of your data\n",
        "2. Handles missing data (imputation)\n",
        "3. Encodes categorical variables\n",
        "4. Scales features\n",
        "5. Prepares final ML-ready dataset\n",
        "\n",
        "All charts saved to: preprocessing_visualizations/\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ðŸ“Š FINAL PREPROCESSING & DATA VISUALIZATION\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "# Create output directory\n",
        "OUTPUT_DIR = \"preprocessing_visualizations\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 1: LOAD DATA\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 1: LOADING DATA\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "df = pd.read_csv('nhanes_modeling_ready.csv')\n",
        "print(f\"âœ… Loaded {len(df):,} observations\")\n",
        "print(f\"âœ… Variables: {len(df.columns)}\")\n",
        "print()\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 2: DATA OVERVIEW VISUALIZATIONS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 2: CREATING DATA OVERVIEW VISUALIZATIONS\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "# ==================== CHART 1: DEMOGRAPHIC OVERVIEW ====================\n",
        "print(\"Creating Chart 1: Demographic Overview...\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "fig.suptitle('DEMOGRAPHIC OVERVIEW', fontsize=16, fontweight='bold', y=0.995)\n",
        "\n",
        "# Age distribution\n",
        "axes[0, 0].hist(df['Age'].dropna(), bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
        "axes[0, 0].axvline(df['Age'].mean(), color='red', linestyle='--', linewidth=2, label=f\"Mean: {df['Age'].mean():.1f}\")\n",
        "axes[0, 0].set_xlabel('Age (years)', fontsize=11)\n",
        "axes[0, 0].set_ylabel('Count', fontsize=11)\n",
        "axes[0, 0].set_title('Age Distribution', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(alpha=0.3)\n",
        "\n",
        "# Gender distribution\n",
        "gender_counts = df['Gender'].value_counts()\n",
        "gender_labels = ['Male', 'Female']\n",
        "axes[0, 1].pie(gender_counts, labels=gender_labels, autopct='%1.1f%%',\n",
        "               colors=['lightblue', 'lightcoral'], startangle=90)\n",
        "axes[0, 1].set_title('Gender Distribution', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Race distribution\n",
        "if 'Race' in df.columns:\n",
        "    race_counts = df['Race'].value_counts().head(5)\n",
        "    race_labels = {1: 'Mexican American', 2: 'Other Hispanic', 3: 'White', 4: 'Black', 5: 'Other'}\n",
        "    race_names = [race_labels.get(int(k), f'Race {k}') for k in race_counts.index]\n",
        "    axes[0, 2].barh(race_names, race_counts.values, color='coral', edgecolor='black')\n",
        "    axes[0, 2].set_xlabel('Count', fontsize=11)\n",
        "    axes[0, 2].set_title('Race/Ethnicity Distribution', fontsize=12, fontweight='bold')\n",
        "    axes[0, 2].grid(axis='x', alpha=0.3)\n",
        "\n",
        "# BMI distribution\n",
        "axes[1, 0].hist(df['BMXBMI'].dropna(), bins=40, color='orange', edgecolor='black', alpha=0.7)\n",
        "axes[1, 0].axvline(df['BMXBMI'].mean(), color='red', linestyle='--', linewidth=2, label=f\"Mean: {df['BMXBMI'].mean():.1f}\")\n",
        "axes[1, 0].axvline(25, color='green', linestyle=':', linewidth=2, label='Overweight (25)')\n",
        "axes[1, 0].axvline(30, color='darkred', linestyle=':', linewidth=2, label='Obese (30)')\n",
        "axes[1, 0].set_xlabel('BMI', fontsize=11)\n",
        "axes[1, 0].set_ylabel('Count', fontsize=11)\n",
        "axes[1, 0].set_title('BMI Distribution', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(alpha=0.3)\n",
        "\n",
        "# BMI Category\n",
        "bmi_cat_counts = df['BMI_Category'].value_counts()\n",
        "axes[1, 1].bar(bmi_cat_counts.index, bmi_cat_counts.values,\n",
        "               color=['lightblue', 'lightgreen', 'orange', 'red'], edgecolor='black')\n",
        "axes[1, 1].set_xlabel('Category', fontsize=11)\n",
        "axes[1, 1].set_ylabel('Count', fontsize=11)\n",
        "axes[1, 1].set_title('BMI Categories', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "axes[1, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Age Group\n",
        "age_group_counts = df['Age_Group'].value_counts().sort_index()\n",
        "axes[1, 2].bar(range(len(age_group_counts)), age_group_counts.values,\n",
        "               color='teal', edgecolor='black')\n",
        "axes[1, 2].set_xticks(range(len(age_group_counts)))\n",
        "axes[1, 2].set_xticklabels(age_group_counts.index, rotation=45)\n",
        "axes[1, 2].set_xlabel('Age Group', fontsize=11)\n",
        "axes[1, 2].set_ylabel('Count', fontsize=11)\n",
        "axes[1, 2].set_title('Age Group Distribution', fontsize=12, fontweight='bold')\n",
        "axes[1, 2].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{OUTPUT_DIR}/01_demographic_overview.png\", dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f\"âœ… Saved: {OUTPUT_DIR}/01_demographic_overview.png\")\n",
        "print()\n",
        "\n",
        "# ==================== CHART 2: DIABETES PREVALENCE ====================\n",
        "print(\"Creating Chart 2: Diabetes Prevalence Analysis...\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "fig.suptitle('DIABETES PREVALENCE ANALYSIS', fontsize=16, fontweight='bold', y=0.995)\n",
        "\n",
        "# Overall prevalence\n",
        "diabetes_counts = df['Has_Diabetes'].value_counts()\n",
        "colors_diab = ['lightgreen', 'crimson']\n",
        "labels_diab = ['No Diabetes', 'Has Diabetes']\n",
        "axes[0, 0].pie(diabetes_counts, labels=labels_diab, autopct='%1.1f%%',\n",
        "               colors=colors_diab, startangle=90, explode=(0, 0.1))\n",
        "axes[0, 0].set_title('Overall Diabetes Prevalence', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Diabetes by age group\n",
        "diabetes_by_age = df.groupby('Age_Group')['Has_Diabetes'].apply(lambda x: (x.sum() / len(x)) * 100)\n",
        "axes[0, 1].bar(range(len(diabetes_by_age)), diabetes_by_age.values,\n",
        "               color='crimson', edgecolor='black', alpha=0.7)\n",
        "axes[0, 1].set_xticks(range(len(diabetes_by_age)))\n",
        "axes[0, 1].set_xticklabels(diabetes_by_age.index, rotation=45)\n",
        "axes[0, 1].set_xlabel('Age Group', fontsize=11)\n",
        "axes[0, 1].set_ylabel('Diabetes Prevalence (%)', fontsize=11)\n",
        "axes[0, 1].set_title('Diabetes Prevalence by Age Group', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Diabetes by BMI category\n",
        "diabetes_by_bmi = df.groupby('BMI_Category')['Has_Diabetes'].apply(lambda x: (x.sum() / len(x)) * 100)\n",
        "axes[1, 0].bar(range(len(diabetes_by_bmi)), diabetes_by_bmi.values,\n",
        "               color=['lightblue', 'lightgreen', 'orange', 'red'], edgecolor='black')\n",
        "axes[1, 0].set_xticks(range(len(diabetes_by_bmi)))\n",
        "axes[1, 0].set_xticklabels(diabetes_by_bmi.index, rotation=45)\n",
        "axes[1, 0].set_xlabel('BMI Category', fontsize=11)\n",
        "axes[1, 0].set_ylabel('Diabetes Prevalence (%)', fontsize=11)\n",
        "axes[1, 0].set_title('Diabetes Prevalence by BMI Category', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Diabetes by gender\n",
        "diabetes_by_gender = df.groupby('Gender')['Has_Diabetes'].apply(lambda x: (x.sum() / len(x)) * 100)\n",
        "gender_labels_bar = ['Male', 'Female']\n",
        "axes[1, 1].bar(gender_labels_bar, diabetes_by_gender.values,\n",
        "               color=['lightblue', 'lightcoral'], edgecolor='black')\n",
        "axes[1, 1].set_xlabel('Gender', fontsize=11)\n",
        "axes[1, 1].set_ylabel('Diabetes Prevalence (%)', fontsize=11)\n",
        "axes[1, 1].set_title('Diabetes Prevalence by Gender', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{OUTPUT_DIR}/02_diabetes_prevalence.png\", dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f\"âœ… Saved: {OUTPUT_DIR}/02_diabetes_prevalence.png\")\n",
        "print()\n",
        "\n",
        "# ==================== CHART 3: KEY BIOMARKERS ====================\n",
        "print(\"Creating Chart 3: Key Biomarker Distributions...\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "fig.suptitle('KEY BIOMARKER DISTRIBUTIONS', fontsize=16, fontweight='bold', y=0.995)\n",
        "\n",
        "# Glucose distribution\n",
        "glucose_data = df['LBXGLU'].dropna()\n",
        "axes[0, 0].hist(glucose_data, bins=50, color='purple', edgecolor='black', alpha=0.7)\n",
        "axes[0, 0].axvline(100, color='green', linestyle='--', linewidth=2, label='Normal (<100)')\n",
        "axes[0, 0].axvline(126, color='red', linestyle='--', linewidth=2, label='Diabetes (â‰¥126)')\n",
        "axes[0, 0].set_xlabel('Fasting Glucose (mg/dL)', fontsize=11)\n",
        "axes[0, 0].set_ylabel('Count', fontsize=11)\n",
        "axes[0, 0].set_title('Glucose Distribution', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(alpha=0.3)\n",
        "\n",
        "# HbA1c distribution\n",
        "hba1c_data = df['LBXGH'].dropna()\n",
        "axes[0, 1].hist(hba1c_data, bins=50, color='darkgreen', edgecolor='black', alpha=0.7)\n",
        "axes[0, 1].axvline(5.7, color='green', linestyle='--', linewidth=2, label='Normal (<5.7)')\n",
        "axes[0, 1].axvline(6.5, color='red', linestyle='--', linewidth=2, label='Diabetes (â‰¥6.5)')\n",
        "axes[0, 1].set_xlabel('HbA1c (%)', fontsize=11)\n",
        "axes[0, 1].set_ylabel('Count', fontsize=11)\n",
        "axes[0, 1].set_title('HbA1c Distribution', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(alpha=0.3)\n",
        "\n",
        "# Glucose vs HbA1c scatter\n",
        "axes[0, 2].scatter(df['LBXGLU'], df['LBXGH'], alpha=0.3, s=10, color='darkblue')\n",
        "axes[0, 2].axvline(126, color='red', linestyle='--', alpha=0.5, label='Glucose=126')\n",
        "axes[0, 2].axhline(6.5, color='red', linestyle='--', alpha=0.5, label='HbA1c=6.5')\n",
        "axes[0, 2].set_xlabel('Glucose (mg/dL)', fontsize=11)\n",
        "axes[0, 2].set_ylabel('HbA1c (%)', fontsize=11)\n",
        "axes[0, 2].set_title('Glucose vs HbA1c Relationship', fontsize=12, fontweight='bold')\n",
        "axes[0, 2].legend()\n",
        "axes[0, 2].grid(alpha=0.3)\n",
        "\n",
        "# Glucose by diabetes status\n",
        "diabetes_glucose = df[df['Has_Diabetes'] == True]['LBXGLU'].dropna()\n",
        "no_diabetes_glucose = df[df['Has_Diabetes'] == False]['LBXGLU'].dropna()\n",
        "axes[1, 0].boxplot([no_diabetes_glucose, diabetes_glucose], labels=['No Diabetes', 'Has Diabetes'])\n",
        "axes[1, 0].set_ylabel('Glucose (mg/dL)', fontsize=11)\n",
        "axes[1, 0].set_title('Glucose by Diabetes Status', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# HbA1c by diabetes status\n",
        "diabetes_hba1c = df[df['Has_Diabetes'] == True]['LBXGH'].dropna()\n",
        "no_diabetes_hba1c = df[df['Has_Diabetes'] == False]['LBXGH'].dropna()\n",
        "axes[1, 1].boxplot([no_diabetes_hba1c, diabetes_hba1c], labels=['No Diabetes', 'Has Diabetes'])\n",
        "axes[1, 1].set_ylabel('HbA1c (%)', fontsize=11)\n",
        "axes[1, 1].set_title('HbA1c by Diabetes Status', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# BMI by diabetes status\n",
        "diabetes_bmi = df[df['Has_Diabetes'] == True]['BMXBMI'].dropna()\n",
        "no_diabetes_bmi = df[df['Has_Diabetes'] == False]['BMXBMI'].dropna()\n",
        "axes[1, 2].boxplot([no_diabetes_bmi, diabetes_bmi], labels=['No Diabetes', 'Has Diabetes'])\n",
        "axes[1, 2].set_ylabel('BMI', fontsize=11)\n",
        "axes[1, 2].set_title('BMI by Diabetes Status', fontsize=12, fontweight='bold')\n",
        "axes[1, 2].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{OUTPUT_DIR}/03_key_biomarkers.png\", dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f\"âœ… Saved: {OUTPUT_DIR}/03_key_biomarkers.png\")\n",
        "print()\n",
        "\n",
        "# ==================== CHART 4: MISSING DATA ANALYSIS ====================\n",
        "print(\"Creating Chart 4: Missing Data Analysis...\")\n",
        "\n",
        "# Select key variables\n",
        "key_vars = ['Age', 'Gender', 'Race', 'BMXBMI', 'LBXGLU', 'LBXGH', 'LBXIN',\n",
        "            'LBXTC', 'LBDLDL', 'LBXTR', 'Income']\n",
        "available_vars = [v for v in key_vars if v in df.columns]\n",
        "\n",
        "missing_data = []\n",
        "for var in available_vars:\n",
        "    missing = df[var].isna().sum()\n",
        "    missing_pct = missing / len(df) * 100\n",
        "    missing_data.append({'Variable': var, 'Missing_Percent': missing_pct})\n",
        "\n",
        "df_missing = pd.DataFrame(missing_data).sort_values('Missing_Percent', ascending=False)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "colors = ['green' if x < 10 else 'orange' if x < 30 else 'red' for x in df_missing['Missing_Percent']]\n",
        "ax.barh(df_missing['Variable'], df_missing['Missing_Percent'], color=colors, edgecolor='black')\n",
        "ax.set_xlabel('Missing Data (%)', fontsize=12)\n",
        "ax.set_title('MISSING DATA ANALYSIS - Key Variables', fontsize=14, fontweight='bold')\n",
        "ax.axvline(10, color='green', linestyle='--', alpha=0.5, label='10% threshold')\n",
        "ax.axvline(30, color='orange', linestyle='--', alpha=0.5, label='30% threshold')\n",
        "ax.legend()\n",
        "ax.grid(axis='x', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{OUTPUT_DIR}/04_missing_data.png\", dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f\"âœ… Saved: {OUTPUT_DIR}/04_missing_data.png\")\n",
        "print()\n",
        "\n",
        "# ==================== CHART 5: CORRELATION HEATMAP ====================\n",
        "print(\"Creating Chart 5: Correlation Heatmap...\")\n",
        "\n",
        "# Select numeric variables for correlation\n",
        "corr_vars = ['Age', 'BMXBMI', 'LBXGLU', 'LBXGH', 'Income']\n",
        "available_corr = [v for v in corr_vars if v in df.columns]\n",
        "\n",
        "if len(available_corr) >= 2:\n",
        "    df_corr = df[available_corr].corr()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    sns.heatmap(df_corr, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
        "                square=True, linewidths=2, cbar_kws={\"shrink\": 0.8}, ax=ax)\n",
        "    ax.set_title('CORRELATION MATRIX - Key Predictors', fontsize=14, fontweight='bold', pad=20)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{OUTPUT_DIR}/05_correlation_heatmap.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"âœ… Saved: {OUTPUT_DIR}/05_correlation_heatmap.png\")\n",
        "print()\n",
        "\n",
        "# ==================== CHART 6: AGE VS KEY BIOMARKERS ====================\n",
        "print(\"Creating Chart 6: Age Relationships...\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "fig.suptitle('AGE RELATIONSHIPS WITH KEY BIOMARKERS', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Age vs BMI\n",
        "axes[0].scatter(df['Age'], df['BMXBMI'], alpha=0.3, s=10, color='orange')\n",
        "axes[0].set_xlabel('Age (years)', fontsize=11)\n",
        "axes[0].set_ylabel('BMI', fontsize=11)\n",
        "axes[0].set_title('Age vs BMI', fontsize=12, fontweight='bold')\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Age vs Glucose\n",
        "axes[1].scatter(df['Age'], df['LBXGLU'], alpha=0.3, s=10, color='purple')\n",
        "axes[1].set_xlabel('Age (years)', fontsize=11)\n",
        "axes[1].set_ylabel('Glucose (mg/dL)', fontsize=11)\n",
        "axes[1].set_title('Age vs Glucose', fontsize=12, fontweight='bold')\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "# Age vs HbA1c\n",
        "axes[2].scatter(df['Age'], df['LBXGH'], alpha=0.3, s=10, color='darkgreen')\n",
        "axes[2].set_xlabel('Age (years)', fontsize=11)\n",
        "axes[2].set_ylabel('HbA1c (%)', fontsize=11)\n",
        "axes[2].set_title('Age vs HbA1c', fontsize=12, fontweight='bold')\n",
        "axes[2].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{OUTPUT_DIR}/06_age_relationships.png\", dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f\"âœ… Saved: {OUTPUT_DIR}/06_age_relationships.png\")\n",
        "print()\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 3: HANDLE MISSING DATA\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 3: HANDLING MISSING DATA\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "print(\"Missing data strategy:\")\n",
        "print(\"   â€¢ <10% missing: Keep as-is or simple imputation\")\n",
        "print(\"   â€¢ 10-50% missing: Median/mode imputation + missing indicator\")\n",
        "print(\"   â€¢ >50% missing: Consider dropping\")\n",
        "print()\n",
        "\n",
        "# Create a copy for preprocessing\n",
        "df_processed = df.copy()\n",
        "\n",
        "# Impute numeric variables with median\n",
        "numeric_vars_to_impute = ['LBXGLU', 'BMXBMI', 'LBXGH', 'Income']\n",
        "\n",
        "for var in numeric_vars_to_impute:\n",
        "    if var in df_processed.columns:\n",
        "        missing_before = df_processed[var].isna().sum()\n",
        "        if missing_before > 0:\n",
        "            # Create missing indicator\n",
        "            df_processed[f'{var}_Missing'] = df_processed[var].isna().astype(int)\n",
        "\n",
        "            # Impute with median\n",
        "            median_val = df_processed[var].median()\n",
        "            df_processed[var].fillna(median_val, inplace=True)\n",
        "\n",
        "            print(f\"âœ… {var}: Imputed {missing_before:,} missing values with median ({median_val:.1f})\")\n",
        "\n",
        "print()\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 4: ENCODE CATEGORICAL VARIABLES\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 4: ENCODING CATEGORICAL VARIABLES\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "# Gender: 1=Male, 2=Female â†’ 0=Male, 1=Female\n",
        "if 'Gender' in df_processed.columns:\n",
        "    df_processed['Gender_Female'] = (df_processed['Gender'] == 2).astype(int)\n",
        "    print(\"âœ… Gender encoded: Gender_Female (0=Male, 1=Female)\")\n",
        "\n",
        "# Race: One-hot encoding\n",
        "if 'Race' in df_processed.columns:\n",
        "    race_dummies = pd.get_dummies(df_processed['Race'], prefix='Race', drop_first=True)\n",
        "    df_processed = pd.concat([df_processed, race_dummies], axis=1)\n",
        "    print(f\"âœ… Race one-hot encoded: {len(race_dummies.columns)} categories\")\n",
        "\n",
        "# BMI Category: One-hot encoding\n",
        "if 'BMI_Category' in df_processed.columns:\n",
        "    bmi_cat_dummies = pd.get_dummies(df_processed['BMI_Category'], prefix='BMI_Cat', drop_first=True)\n",
        "    df_processed = pd.concat([df_processed, bmi_cat_dummies], axis=1)\n",
        "    print(f\"âœ… BMI Category encoded: {len(bmi_cat_dummies.columns)} categories\")\n",
        "\n",
        "# Age Group: One-hot encoding\n",
        "if 'Age_Group' in df_processed.columns:\n",
        "    age_group_dummies = pd.get_dummies(df_processed['Age_Group'], prefix='Age_Group', drop_first=True)\n",
        "    df_processed = pd.concat([df_processed, age_group_dummies], axis=1)\n",
        "    print(f\"âœ… Age Group encoded: {len(age_group_dummies.columns)} categories\")\n",
        "\n",
        "print()\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 5: FEATURE SCALING\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 5: FEATURE SCALING (Standardization)\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "# Variables to scale\n",
        "vars_to_scale = ['Age', 'BMXBMI', 'LBXGLU', 'LBXGH', 'Income']\n",
        "available_to_scale = [v for v in vars_to_scale if v in df_processed.columns]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "for var in available_to_scale:\n",
        "    df_processed[f'{var}_Scaled'] = scaler.fit_transform(df_processed[[var]])\n",
        "    print(f\"âœ… {var} scaled: mean=0, std=1\")\n",
        "\n",
        "print()\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 6: CREATE FINAL FEATURE SET\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 6: CREATING FINAL FEATURE SET\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "# Define feature groups\n",
        "feature_groups = {\n",
        "    'Demographics': ['Age', 'Gender_Female'] + [c for c in df_processed.columns if 'Race_' in c],\n",
        "    'Anthropometric': ['BMXBMI'] + [c for c in df_processed.columns if 'BMI_Cat_' in c],\n",
        "    'Biomarkers': ['LBXGLU', 'LBXGH'],\n",
        "    'Socioeconomic': ['Income'],\n",
        "    'Missing_Indicators': [c for c in df_processed.columns if '_Missing' in c],\n",
        "}\n",
        "\n",
        "# Print feature summary\n",
        "print(\"Feature groups:\")\n",
        "for group, features in feature_groups.items():\n",
        "    available = [f for f in features if f in df_processed.columns]\n",
        "    print(f\"   â€¢ {group}: {len(available)} features\")\n",
        "    for f in available[:3]:\n",
        "        print(f\"      - {f}\")\n",
        "    if len(available) > 3:\n",
        "        print(f\"      ... and {len(available)-3} more\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Create final feature list\n",
        "all_features = []\n",
        "for features in feature_groups.values():\n",
        "    all_features.extend([f for f in features if f in df_processed.columns])\n",
        "\n",
        "# Remove duplicates\n",
        "all_features = list(dict.fromkeys(all_features))\n",
        "\n",
        "print(f\"ðŸ“Š Total features for modeling: {len(all_features)}\")\n",
        "print()\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 7: SAVE PREPROCESSED DATA\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 7: SAVING PREPROCESSED DATA\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "# Save full preprocessed dataset\n",
        "df_processed.to_csv('nhanes_ml_ready.csv', index=False)\n",
        "print(f\"âœ… Saved: nhanes_ml_ready.csv\")\n",
        "print(f\"   â€¢ {len(df_processed):,} observations\")\n",
        "print(f\"   â€¢ {len(df_processed.columns)} total variables\")\n",
        "print()\n",
        "\n",
        "# Create and save feature matrix and target\n",
        "X = df_processed[all_features]\n",
        "y = df_processed['Has_Diabetes'].astype(int)\n",
        "\n",
        "# Save separately for easy loading\n",
        "X.to_csv('X_features.csv', index=False)\n",
        "y.to_csv('y_target.csv', index=False, header=True)\n",
        "\n",
        "print(f\"âœ… Saved: X_features.csv\")\n",
        "print(f\"   â€¢ {X.shape[0]:,} observations\")\n",
        "print(f\"   â€¢ {X.shape[1]} features\")\n",
        "print()\n",
        "\n",
        "print(f\"âœ… Saved: y_target.csv\")\n",
        "print(f\"   â€¢ {y.sum():,} positive cases ({y.sum()/len(y)*100:.1f}%)\")\n",
        "print(f\"   â€¢ {len(y)-y.sum():,} negative cases ({(len(y)-y.sum())/len(y)*100:.1f}%)\")\n",
        "print()\n",
        "\n",
        "# Save feature names for reference\n",
        "feature_info = pd.DataFrame({\n",
        "    'Feature_Name': all_features,\n",
        "    'Data_Type': [df_processed[f].dtype for f in all_features],\n",
        "    'Missing_Count': [df_processed[f].isna().sum() for f in all_features]\n",
        "})\n",
        "feature_info.to_csv('feature_list.csv', index=False)\n",
        "print(f\"âœ… Saved: feature_list.csv (feature reference)\")\n",
        "print()\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 8: FINAL SUMMARY\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ðŸ“Š FINAL SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "print(\"VISUALIZATIONS CREATED:\")\n",
        "print(\"   1. 01_demographic_overview.png - Age, gender, race, BMI distributions\")\n",
        "print(\"   2. 02_diabetes_prevalence.png - Prevalence by age, BMI, gender\")\n",
        "print(\"   3. 03_key_biomarkers.png - Glucose, HbA1c distributions and relationships\")\n",
        "print(\"   4. 04_missing_data.png - Missing data analysis\")\n",
        "print(\"   5. 05_correlation_heatmap.png - Feature correlations\")\n",
        "print(\"   6. 06_age_relationships.png - Age vs biomarkers\")\n",
        "print()\n",
        "\n",
        "print(\"DATA FILES CREATED:\")\n",
        "print(\"   â€¢ nhanes_ml_ready.csv - Full preprocessed dataset\")\n",
        "print(\"   â€¢ X_features.csv - Feature matrix (ready for ML)\")\n",
        "print(\"   â€¢ y_target.csv - Target variable\")\n",
        "print(\"   â€¢ feature_list.csv - Feature reference guide\")\n",
        "print()\n",
        "\n",
        "print(\"PREPROCESSING COMPLETE:\")\n",
        "print(f\"   âœ… {len(df_processed):,} observations ready\")\n",
        "print(f\"   âœ… {len(all_features)} features prepared\")\n",
        "print(f\"   âœ… Missing data handled\")\n",
        "print(f\"   âœ… Categorical variables encoded\")\n",
        "print(f\"   âœ… Features scaled\")\n",
        "print(f\"   âœ… Target: {y.sum():,} diabetes cases ({y.sum()/len(y)*100:.1f}%)\")\n",
        "print()\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"âœ… ALL PREPROCESSING COMPLETE!\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "print(\"ðŸŽ‰ Your data is 100% ready for machine learning!\")\n",
        "print()\n",
        "print(\"Next steps:\")\n",
        "print(\"   1. Review the 6 visualization charts\")\n",
        "print(\"   2. Load X_features.csv and y_target.csv\")\n",
        "print(\"   3. Split into train/test sets\")\n",
        "print(\"   4. Build your first model!\")\n",
        "print()\n",
        "print(\"Example code to start modeling:\")\n",
        "print(\"=\" * 40)\n",
        "print(\"\"\"\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load data\n",
        "X = pd.read_csv('X_features.csv')\n",
        "y = pd.read_csv('y_target.csv')['Has_Diabetes']\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Train\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "score = model.score(X_test, y_test)\n",
        "print(f'Accuracy: {score:.3f}')\n",
        "\"\"\")\n",
        "print(\"=\" * 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZhWQF8E4mlf",
        "outputId": "c78e725b-8bc4-4e97-f4f6-a21be12c0de8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ðŸ“Š FINAL PREPROCESSING & DATA VISUALIZATION\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "STEP 1: LOADING DATA\n",
            "================================================================================\n",
            "\n",
            "âœ… Loaded 62,238 observations\n",
            "âœ… Variables: 425\n",
            "\n",
            "================================================================================\n",
            "STEP 2: CREATING DATA OVERVIEW VISUALIZATIONS\n",
            "================================================================================\n",
            "\n",
            "Creating Chart 1: Demographic Overview...\n",
            "âœ… Saved: preprocessing_visualizations/01_demographic_overview.png\n",
            "\n",
            "Creating Chart 2: Diabetes Prevalence Analysis...\n",
            "âœ… Saved: preprocessing_visualizations/02_diabetes_prevalence.png\n",
            "\n",
            "Creating Chart 3: Key Biomarker Distributions...\n",
            "âœ… Saved: preprocessing_visualizations/03_key_biomarkers.png\n",
            "\n",
            "Creating Chart 4: Missing Data Analysis...\n",
            "âœ… Saved: preprocessing_visualizations/04_missing_data.png\n",
            "\n",
            "Creating Chart 5: Correlation Heatmap...\n",
            "âœ… Saved: preprocessing_visualizations/05_correlation_heatmap.png\n",
            "\n",
            "Creating Chart 6: Age Relationships...\n",
            "âœ… Saved: preprocessing_visualizations/06_age_relationships.png\n",
            "\n",
            "================================================================================\n",
            "STEP 3: HANDLING MISSING DATA\n",
            "================================================================================\n",
            "\n",
            "Missing data strategy:\n",
            "   â€¢ <10% missing: Keep as-is or simple imputation\n",
            "   â€¢ 10-50% missing: Median/mode imputation + missing indicator\n",
            "   â€¢ >50% missing: Consider dropping\n",
            "\n",
            "âœ… LBXGLU: Imputed 33,034 missing values with median (99.0)\n",
            "âœ… BMXBMI: Imputed 6,052 missing values with median (27.8)\n",
            "âœ… LBXGH: Imputed 2,853 missing values with median (5.5)\n",
            "âœ… Income: Imputed 5,925 missing values with median (2.1)\n",
            "\n",
            "================================================================================\n",
            "STEP 4: ENCODING CATEGORICAL VARIABLES\n",
            "================================================================================\n",
            "\n",
            "âœ… Gender encoded: Gender_Female (0=Male, 1=Female)\n",
            "âœ… Race one-hot encoded: 4 categories\n",
            "âœ… BMI Category encoded: 3 categories\n",
            "âœ… Age Group encoded: 5 categories\n",
            "\n",
            "================================================================================\n",
            "STEP 5: FEATURE SCALING (Standardization)\n",
            "================================================================================\n",
            "\n",
            "âœ… Age scaled: mean=0, std=1\n",
            "âœ… BMXBMI scaled: mean=0, std=1\n",
            "âœ… LBXGLU scaled: mean=0, std=1\n",
            "âœ… LBXGH scaled: mean=0, std=1\n",
            "âœ… Income scaled: mean=0, std=1\n",
            "\n",
            "================================================================================\n",
            "STEP 6: CREATING FINAL FEATURE SET\n",
            "================================================================================\n",
            "\n",
            "Feature groups:\n",
            "   â€¢ Demographics: 6 features\n",
            "      - Age\n",
            "      - Gender_Female\n",
            "      - Race_2.0\n",
            "      ... and 3 more\n",
            "   â€¢ Anthropometric: 4 features\n",
            "      - BMXBMI\n",
            "      - BMI_Cat_Obese\n",
            "      - BMI_Cat_Overweight\n",
            "      ... and 1 more\n",
            "   â€¢ Biomarkers: 2 features\n",
            "      - LBXGLU\n",
            "      - LBXGH\n",
            "   â€¢ Socioeconomic: 1 features\n",
            "      - Income\n",
            "   â€¢ Missing_Indicators: 4 features\n",
            "      - LBXGLU_Missing\n",
            "      - BMXBMI_Missing\n",
            "      - LBXGH_Missing\n",
            "      ... and 1 more\n",
            "\n",
            "ðŸ“Š Total features for modeling: 17\n",
            "\n",
            "================================================================================\n",
            "STEP 7: SAVING PREPROCESSED DATA\n",
            "================================================================================\n",
            "\n",
            "âœ… Saved: nhanes_ml_ready.csv\n",
            "   â€¢ 62,238 observations\n",
            "   â€¢ 447 total variables\n",
            "\n",
            "âœ… Saved: X_features.csv\n",
            "   â€¢ 62,238 observations\n",
            "   â€¢ 17 features\n",
            "\n",
            "âœ… Saved: y_target.csv\n",
            "   â€¢ 9,527 positive cases (15.3%)\n",
            "   â€¢ 52,711 negative cases (84.7%)\n",
            "\n",
            "âœ… Saved: feature_list.csv (feature reference)\n",
            "\n",
            "================================================================================\n",
            "ðŸ“Š FINAL SUMMARY\n",
            "================================================================================\n",
            "\n",
            "VISUALIZATIONS CREATED:\n",
            "   1. 01_demographic_overview.png - Age, gender, race, BMI distributions\n",
            "   2. 02_diabetes_prevalence.png - Prevalence by age, BMI, gender\n",
            "   3. 03_key_biomarkers.png - Glucose, HbA1c distributions and relationships\n",
            "   4. 04_missing_data.png - Missing data analysis\n",
            "   5. 05_correlation_heatmap.png - Feature correlations\n",
            "   6. 06_age_relationships.png - Age vs biomarkers\n",
            "\n",
            "DATA FILES CREATED:\n",
            "   â€¢ nhanes_ml_ready.csv - Full preprocessed dataset\n",
            "   â€¢ X_features.csv - Feature matrix (ready for ML)\n",
            "   â€¢ y_target.csv - Target variable\n",
            "   â€¢ feature_list.csv - Feature reference guide\n",
            "\n",
            "PREPROCESSING COMPLETE:\n",
            "   âœ… 62,238 observations ready\n",
            "   âœ… 17 features prepared\n",
            "   âœ… Missing data handled\n",
            "   âœ… Categorical variables encoded\n",
            "   âœ… Features scaled\n",
            "   âœ… Target: 9,527 diabetes cases (15.3%)\n",
            "\n",
            "================================================================================\n",
            "âœ… ALL PREPROCESSING COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "ðŸŽ‰ Your data is 100% ready for machine learning!\n",
            "\n",
            "Next steps:\n",
            "   1. Review the 6 visualization charts\n",
            "   2. Load X_features.csv and y_target.csv\n",
            "   3. Split into train/test sets\n",
            "   4. Build your first model!\n",
            "\n",
            "Example code to start modeling:\n",
            "========================================\n",
            "\n",
            "import pandas as pd\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.linear_model import LogisticRegression\n",
            "\n",
            "# Load data\n",
            "X = pd.read_csv('X_features.csv')\n",
            "y = pd.read_csv('y_target.csv')['Has_Diabetes']\n",
            "\n",
            "# Split\n",
            "X_train, X_test, y_train, y_test = train_test_split(\n",
            "    X, y, test_size=0.2, random_state=42, stratify=y\n",
            ")\n",
            "\n",
            "# Train\n",
            "model = LogisticRegression(max_iter=1000)\n",
            "model.fit(X_train, y_train)\n",
            "\n",
            "# Evaluate\n",
            "score = model.score(X_test, y_test)\n",
            "print(f'Accuracy: {score:.3f}')\n",
            "\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ZIP ESSENTIAL FILES FOR GOOGLE DRIVE BACKUP\n",
        "============================================\n",
        "This script identifies and zips the most important files from your analysis:\n",
        "- Final modeling data\n",
        "- Visualizations\n",
        "- Reports and documentation\n",
        "- Reference files\n",
        "\n",
        "Organized into logical ZIP files for easy management.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "from datetime import datetime\n",
        "import shutil\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ðŸ“¦ CREATING BACKUP ZIP FILES\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "# Create backup directory\n",
        "BACKUP_DIR = \"nhanes_diabetes_backup\"\n",
        "os.makedirs(BACKUP_DIR, exist_ok=True)\n",
        "\n",
        "# =============================================================================\n",
        "# FILE CATEGORIES\n",
        "# =============================================================================\n",
        "\n",
        "file_categories = {\n",
        "\n",
        "    # ESSENTIAL DATA FILES - The final clean data you worked hard to create\n",
        "    'essential_data': {\n",
        "        'files': [\n",
        "            'nhanes_modeling_ready.csv',      # Final clean data (62K obs, adults only)\n",
        "            'nhanes_ml_ready.csv',            # Preprocessed for ML\n",
        "            'X_features.csv',                  # Feature matrix\n",
        "            'y_target.csv',                    # Target variable\n",
        "            'feature_list.csv',                # Feature reference\n",
        "        ],\n",
        "        'description': 'Final modeling-ready datasets'\n",
        "    },\n",
        "\n",
        "    # VISUALIZATIONS - Charts for reports/presentations\n",
        "    'visualizations': {\n",
        "        'folders': [\n",
        "            'eda_longitudinal_outputs',\n",
        "            'preprocessing_visualizations'\n",
        "        ],\n",
        "        'description': 'All EDA and preprocessing charts'\n",
        "    },\n",
        "\n",
        "    # ENHANCED DATA - With demographics added (larger file)\n",
        "    'enhanced_data': {\n",
        "        'files': [\n",
        "            'nhanes_longitudinal_enhanced.csv',  # With demographics (135K obs)\n",
        "        ],\n",
        "        'description': 'Longitudinal data with demographics (all observations)'\n",
        "    },\n",
        "\n",
        "    # ORIGINAL WIDE FORMAT - The source data (LARGEST)\n",
        "    'source_data': {\n",
        "        'files': [\n",
        "            'nhanes_patient_flattened_enhanced.csv',  # Original wide format\n",
        "        ],\n",
        "        'description': 'Original wide-format data from NHANES extractor (LARGE!)'\n",
        "    },\n",
        "\n",
        "    # REPORTS AND DOCUMENTATION\n",
        "    'reports': {\n",
        "        'files': [\n",
        "            'eda_longitudinal_outputs/eda_summary_report.txt',\n",
        "            'eda_longitudinal_outputs/univariate_statistics.csv',\n",
        "            'eda_longitudinal_outputs/outliers.csv',\n",
        "        ],\n",
        "        'description': 'Summary reports and statistics'\n",
        "    },\n",
        "}\n",
        "\n",
        "# =============================================================================\n",
        "# FUNCTION TO ZIP FILES\n",
        "# =============================================================================\n",
        "\n",
        "def create_zip(zip_name, files=None, folders=None, description=\"\"):\n",
        "    \"\"\"Create a zip file with specified files and folders.\"\"\"\n",
        "\n",
        "    zip_path = os.path.join(BACKUP_DIR, zip_name)\n",
        "\n",
        "    print(f\"Creating: {zip_name}\")\n",
        "    print(f\"   Purpose: {description}\")\n",
        "\n",
        "    files_added = 0\n",
        "    total_size = 0\n",
        "\n",
        "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "\n",
        "        # Add individual files\n",
        "        if files:\n",
        "            for file in files:\n",
        "                if os.path.exists(file):\n",
        "                    file_size = os.path.getsize(file)\n",
        "                    zipf.write(file, os.path.basename(file))\n",
        "                    files_added += 1\n",
        "                    total_size += file_size\n",
        "                    print(f\"   âœ… Added: {file} ({file_size / 1024 / 1024:.1f} MB)\")\n",
        "                else:\n",
        "                    print(f\"   âš ï¸  Not found: {file}\")\n",
        "\n",
        "        # Add folders\n",
        "        if folders:\n",
        "            for folder in folders:\n",
        "                if os.path.exists(folder):\n",
        "                    for root, dirs, files_in_folder in os.walk(folder):\n",
        "                        for file in files_in_folder:\n",
        "                            file_path = os.path.join(root, file)\n",
        "                            arcname = os.path.relpath(file_path)\n",
        "                            file_size = os.path.getsize(file_path)\n",
        "                            zipf.write(file_path, arcname)\n",
        "                            files_added += 1\n",
        "                            total_size += file_size\n",
        "                    print(f\"   âœ… Added folder: {folder}/\")\n",
        "                else:\n",
        "                    print(f\"   âš ï¸  Not found: {folder}\")\n",
        "\n",
        "    if files_added > 0:\n",
        "        zip_size = os.path.getsize(zip_path)\n",
        "        print(f\"   ðŸ“¦ ZIP created: {files_added} items, {zip_size / 1024 / 1024:.1f} MB\")\n",
        "        return zip_size\n",
        "    else:\n",
        "        os.remove(zip_path)\n",
        "        print(f\"   âŒ No files found - ZIP not created\")\n",
        "        return 0\n",
        "\n",
        "    print()\n",
        "\n",
        "# =============================================================================\n",
        "# CREATE ZIP FILES\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Creating organized ZIP files...\")\n",
        "print()\n",
        "\n",
        "total_backup_size = 0\n",
        "\n",
        "# 1. ESSENTIAL DATA (Small, most important)\n",
        "print(\"â”€\" * 80)\n",
        "print(\"1ï¸âƒ£  ESSENTIAL DATA FILES\")\n",
        "print(\"â”€\" * 80)\n",
        "size = create_zip(\n",
        "    'nhanes_essential_data.zip',\n",
        "    files=file_categories['essential_data']['files'],\n",
        "    description=file_categories['essential_data']['description']\n",
        ")\n",
        "total_backup_size += size\n",
        "print()\n",
        "\n",
        "# 2. VISUALIZATIONS (Medium size, for presentations)\n",
        "print(\"â”€\" * 80)\n",
        "print(\"2ï¸âƒ£  VISUALIZATIONS\")\n",
        "print(\"â”€\" * 80)\n",
        "size = create_zip(\n",
        "    'nhanes_visualizations.zip',\n",
        "    folders=file_categories['visualizations']['folders'],\n",
        "    description=file_categories['visualizations']['description']\n",
        ")\n",
        "total_backup_size += size\n",
        "print()\n",
        "\n",
        "# 3. ENHANCED DATA (Medium-large)\n",
        "print(\"â”€\" * 80)\n",
        "print(\"3ï¸âƒ£  ENHANCED LONGITUDINAL DATA\")\n",
        "print(\"â”€\" * 80)\n",
        "size = create_zip(\n",
        "    'nhanes_enhanced_data.zip',\n",
        "    files=file_categories['enhanced_data']['files'],\n",
        "    description=file_categories['enhanced_data']['description']\n",
        ")\n",
        "total_backup_size += size\n",
        "print()\n",
        "\n",
        "# 4. REPORTS (Small)\n",
        "print(\"â”€\" * 80)\n",
        "print(\"4ï¸âƒ£  REPORTS AND DOCUMENTATION\")\n",
        "print(\"â”€\" * 80)\n",
        "size = create_zip(\n",
        "    'nhanes_reports.zip',\n",
        "    files=file_categories['reports']['files'],\n",
        "    description=file_categories['reports']['description']\n",
        ")\n",
        "total_backup_size += size\n",
        "print()\n",
        "\n",
        "# 5. SOURCE DATA (LARGE - optional)\n",
        "print(\"â”€\" * 80)\n",
        "print(\"5ï¸âƒ£  SOURCE DATA (OPTIONAL - LARGE FILE)\")\n",
        "print(\"â”€\" * 80)\n",
        "print(\"âš ï¸  This is the original wide-format data (can be 500+ MB)\")\n",
        "print(\"   Only create this if you want to backup the source data\")\n",
        "print()\n",
        "\n",
        "if os.path.exists('nhanes_patient_flattened_enhanced.csv'):\n",
        "    source_size = os.path.getsize('nhanes_patient_flattened_enhanced.csv') / 1024 / 1024\n",
        "    print(f\"   Source file size: {source_size:.1f} MB\")\n",
        "    print()\n",
        "\n",
        "    # Only create if < 500 MB (reasonable for Drive)\n",
        "    if source_size < 500:\n",
        "        print(\"   Creating ZIP (file is reasonable size)...\")\n",
        "        size = create_zip(\n",
        "            'nhanes_source_data.zip',\n",
        "            files=file_categories['source_data']['files'],\n",
        "            description=file_categories['source_data']['description']\n",
        "        )\n",
        "        total_backup_size += size\n",
        "    else:\n",
        "        print(\"   âš ï¸  File is very large - skipping to save space\")\n",
        "        print(\"   (You can regenerate this from NHANES if needed)\")\n",
        "else:\n",
        "    print(\"   Source file not found - skipping\")\n",
        "\n",
        "print()\n",
        "\n",
        "# =============================================================================\n",
        "# CREATE README FILE\n",
        "# =============================================================================\n",
        "\n",
        "print(\"â”€\" * 80)\n",
        "print(\"ðŸ“„ CREATING README\")\n",
        "print(\"â”€\" * 80)\n",
        "print()\n",
        "\n",
        "readme_content = f\"\"\"\n",
        "NHANES DIABETES PREDICTION - BACKUP FILES\n",
        "==========================================\n",
        "Created: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "This backup contains all essential files from your NHANES diabetes prediction analysis.\n",
        "\n",
        "ZIP FILES INCLUDED\n",
        "------------------\n",
        "\n",
        "1. nhanes_essential_data.zip (MOST IMPORTANT)\n",
        "   âœ“ nhanes_modeling_ready.csv - Final clean dataset (62,238 adults)\n",
        "   âœ“ nhanes_ml_ready.csv - Preprocessed for machine learning\n",
        "   âœ“ X_features.csv - Feature matrix (~30 features)\n",
        "   âœ“ y_target.csv - Target variable (Has_Diabetes)\n",
        "   âœ“ feature_list.csv - Feature reference guide\n",
        "\n",
        "   â†’ USE THIS to continue modeling immediately\n",
        "\n",
        "2. nhanes_visualizations.zip\n",
        "   âœ“ All EDA charts (demographic overview, diabetes prevalence, biomarkers)\n",
        "   âœ“ Preprocessing visualizations (correlation heatmap, missing data, etc.)\n",
        "\n",
        "   â†’ USE THIS for reports and presentations\n",
        "\n",
        "3. nhanes_enhanced_data.zip\n",
        "   âœ“ nhanes_longitudinal_enhanced.csv - All observations with demographics\n",
        "\n",
        "   â†’ USE THIS if you want to redo preprocessing or try different filters\n",
        "\n",
        "4. nhanes_reports.zip\n",
        "   âœ“ EDA summary report\n",
        "   âœ“ Univariate statistics\n",
        "   âœ“ Outlier analysis\n",
        "\n",
        "   â†’ USE THIS for documentation and reference\n",
        "\n",
        "5. nhanes_source_data.zip (if included)\n",
        "   âœ“ Original wide-format data from NHANES extractor\n",
        "\n",
        "   â†’ USE THIS if you need to regenerate everything from scratch\n",
        "\n",
        "DATASET SUMMARY\n",
        "---------------\n",
        "â€¢ Total patients: 62,238 adults (age 18-85)\n",
        "â€¢ Features: ~30 (demographics, BMI, glucose, HbA1c, etc.)\n",
        "â€¢ Target: Has_Diabetes (15.3% prevalence, 9,527 cases)\n",
        "â€¢ Data completeness: Age/Gender 100%, HbA1c 95%, BMI 90%, Glucose 47%\n",
        "â€¢ Missing data: Imputed with median + indicator variables\n",
        "â€¢ Categorical variables: One-hot encoded\n",
        "â€¢ Numeric variables: Standardized (mean=0, std=1)\n",
        "\n",
        "PIPELINE SUMMARY\n",
        "----------------\n",
        "1. NHANES Extractor â†’ nhanes_patient_flattened_enhanced.csv (wide format)\n",
        "2. Ultra-Fast Reshaper â†’ nhanes_longitudinal.csv (long format)\n",
        "3. Fix & Prepare â†’ nhanes_modeling_ready.csv (clean, adults only)\n",
        "4. Preprocessing â†’ nhanes_ml_ready.csv (ML-ready)\n",
        "\n",
        "NEXT STEPS\n",
        "----------\n",
        "To continue your analysis:\n",
        "\n",
        "1. Upload nhanes_essential_data.zip to new Colab session\n",
        "2. Unzip and load data:\n",
        "\n",
        "   import pandas as pd\n",
        "   X = pd.read_csv('X_features.csv')\n",
        "   y = pd.read_csv('y_target.csv')['Has_Diabetes']\n",
        "\n",
        "3. Split train/test and build models:\n",
        "\n",
        "   from sklearn.model_selection import train_test_split\n",
        "   X_train, X_test, y_train, y_test = train_test_split(\n",
        "       X, y, test_size=0.2, random_state=42, stratify=y\n",
        "   )\n",
        "\n",
        "4. Train and evaluate!\n",
        "\n",
        "SCRIPTS USED\n",
        "------------\n",
        "â€¢ nhanes_extractor.py - Downloaded and flattened NHANES data\n",
        "â€¢ ultra_fast_nhanes_reshaper.py - Reshaped to longitudinal format\n",
        "â€¢ fix_and_prepare_modeling_data.py - Added demographics and cleaned data\n",
        "â€¢ final_preprocessing_and_visualization.py - Preprocessing and visualizations\n",
        "\n",
        "IMPORTANT NOTES\n",
        "---------------\n",
        "â€¢ All data is from NHANES public use files (https://wwwn.cdc.gov/nchs/nhanes/)\n",
        "â€¢ Target population: US adults age 18-85\n",
        "â€¢ Diabetes defined by: Self-report OR Glucose â‰¥126 OR HbA1c â‰¥6.5\n",
        "â€¢ Adult-only focus (pediatric diabetes is a different disease - Type 1)\n",
        "â€¢ Cross-sectional analysis (first observations only)\n",
        "\n",
        "CONTACT & SUPPORT\n",
        "-----------------\n",
        "For questions about NHANES data: https://wwwn.cdc.gov/nchs/nhanes/\n",
        "For questions about this analysis: Review the EDA reports in nhanes_reports.zip\n",
        "\n",
        "Good luck with your diabetes prediction model! ðŸŽ‰\n",
        "\"\"\"\n",
        "\n",
        "readme_path = os.path.join(BACKUP_DIR, 'README.txt')\n",
        "with open(readme_path, 'w') as f:\n",
        "    f.write(readme_content)\n",
        "\n",
        "print(\"âœ… Created: README.txt\")\n",
        "print()\n",
        "\n",
        "# =============================================================================\n",
        "# FINAL SUMMARY\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ðŸ“Š BACKUP SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "backup_files = [f for f in os.listdir(BACKUP_DIR) if f.endswith('.zip')]\n",
        "print(f\"Created {len(backup_files)} ZIP files:\")\n",
        "print()\n",
        "\n",
        "for file in sorted(backup_files):\n",
        "    file_path = os.path.join(BACKUP_DIR, file)\n",
        "    file_size = os.path.getsize(file_path) / 1024 / 1024\n",
        "    print(f\"   ðŸ“¦ {file:35} {file_size:6.1f} MB\")\n",
        "\n",
        "print()\n",
        "print(f\"Total backup size: {total_backup_size / 1024 / 1024:.1f} MB\")\n",
        "print()\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"âœ… BACKUP COMPLETE!\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "print(\"ðŸ“ All files saved to: nhanes_diabetes_backup/\")\n",
        "print()\n",
        "\n",
        "print(\"ðŸŽ¯ RECOMMENDED SAVE ORDER (by priority):\")\n",
        "print()\n",
        "print(\"   1ï¸âƒ£  MUST SAVE:\")\n",
        "print(\"       â€¢ nhanes_essential_data.zip - Your final modeling data\")\n",
        "print(\"       â€¢ nhanes_visualizations.zip - Charts for presentations\")\n",
        "print(\"       â€¢ README.txt - Documentation\")\n",
        "print()\n",
        "print(\"   2ï¸âƒ£  SHOULD SAVE:\")\n",
        "print(\"       â€¢ nhanes_enhanced_data.zip - Full dataset with demographics\")\n",
        "print(\"       â€¢ nhanes_reports.zip - Analysis reports\")\n",
        "print()\n",
        "print(\"   3ï¸âƒ£  OPTIONAL:\")\n",
        "print(\"       â€¢ nhanes_source_data.zip - Original wide format (can regenerate)\")\n",
        "print()\n",
        "\n",
        "print(\"ðŸ’¾ TO SAVE TO GOOGLE DRIVE:\")\n",
        "print()\n",
        "print(\"   Option A: Download individually\")\n",
        "print(\"      â€¢ Right-click each file â†’ Download\")\n",
        "print()\n",
        "print(\"   Option B: Download entire folder\")\n",
        "print(\"      â€¢ Right-click 'nhanes_diabetes_backup' folder â†’ Download\")\n",
        "print()\n",
        "print(\"   Option C: Mount Drive and copy\")\n",
        "print(\"      from google.colab import drive\")\n",
        "print(\"      drive.mount('/content/drive')\")\n",
        "print(\"      !cp -r nhanes_diabetes_backup /content/drive/MyDrive/\")\n",
        "print()\n",
        "\n",
        "print(\"ðŸŽ‰ You can now continue your work anytime by uploading the ZIPs!\")\n",
        "print()\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPj1UZ2WA4xN",
        "outputId": "81fd6378-10f1-4caf-c1ba-94a4f2b1acd8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ðŸ“¦ CREATING BACKUP ZIP FILES\n",
            "================================================================================\n",
            "\n",
            "Creating organized ZIP files...\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "1ï¸âƒ£  ESSENTIAL DATA FILES\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Creating: nhanes_essential_data.zip\n",
            "   Purpose: Final modeling-ready datasets\n",
            "   âœ… Added: nhanes_modeling_ready.csv (44.8 MB)\n",
            "   âœ… Added: nhanes_ml_ready.csv (55.6 MB)\n",
            "   âœ… Added: X_features.csv (4.5 MB)\n",
            "   âœ… Added: y_target.csv (0.1 MB)\n",
            "   âœ… Added: feature_list.csv (0.0 MB)\n",
            "   ðŸ“¦ ZIP created: 5 items, 17.5 MB\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "2ï¸âƒ£  VISUALIZATIONS\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Creating: nhanes_visualizations.zip\n",
            "   Purpose: All EDA and preprocessing charts\n",
            "   âš ï¸  Not found: eda_longitudinal_outputs\n",
            "   âœ… Added folder: preprocessing_visualizations/\n",
            "   ðŸ“¦ ZIP created: 6 items, 3.6 MB\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "3ï¸âƒ£  ENHANCED LONGITUDINAL DATA\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Creating: nhanes_enhanced_data.zip\n",
            "   Purpose: Longitudinal data with demographics (all observations)\n",
            "   âœ… Added: nhanes_longitudinal_enhanced.csv (85.3 MB)\n",
            "   ðŸ“¦ ZIP created: 1 items, 11.4 MB\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "4ï¸âƒ£  REPORTS AND DOCUMENTATION\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Creating: nhanes_reports.zip\n",
            "   Purpose: Summary reports and statistics\n",
            "   âš ï¸  Not found: eda_longitudinal_outputs/eda_summary_report.txt\n",
            "   âš ï¸  Not found: eda_longitudinal_outputs/univariate_statistics.csv\n",
            "   âš ï¸  Not found: eda_longitudinal_outputs/outliers.csv\n",
            "   âŒ No files found - ZIP not created\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "5ï¸âƒ£  SOURCE DATA (OPTIONAL - LARGE FILE)\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "âš ï¸  This is the original wide-format data (can be 500+ MB)\n",
            "   Only create this if you want to backup the source data\n",
            "\n",
            "   Source file size: 313.3 MB\n",
            "\n",
            "   Creating ZIP (file is reasonable size)...\n",
            "Creating: nhanes_source_data.zip\n",
            "   Purpose: Original wide-format data from NHANES extractor (LARGE!)\n",
            "   âœ… Added: nhanes_patient_flattened_enhanced.csv (313.3 MB)\n",
            "   ðŸ“¦ ZIP created: 1 items, 26.5 MB\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ðŸ“„ CREATING README\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "âœ… Created: README.txt\n",
            "\n",
            "================================================================================\n",
            "ðŸ“Š BACKUP SUMMARY\n",
            "================================================================================\n",
            "\n",
            "Created 4 ZIP files:\n",
            "\n",
            "   ðŸ“¦ nhanes_enhanced_data.zip              11.4 MB\n",
            "   ðŸ“¦ nhanes_essential_data.zip             17.5 MB\n",
            "   ðŸ“¦ nhanes_source_data.zip                26.5 MB\n",
            "   ðŸ“¦ nhanes_visualizations.zip              3.6 MB\n",
            "\n",
            "Total backup size: 59.0 MB\n",
            "\n",
            "================================================================================\n",
            "âœ… BACKUP COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "ðŸ“ All files saved to: nhanes_diabetes_backup/\n",
            "\n",
            "ðŸŽ¯ RECOMMENDED SAVE ORDER (by priority):\n",
            "\n",
            "   1ï¸âƒ£  MUST SAVE:\n",
            "       â€¢ nhanes_essential_data.zip - Your final modeling data\n",
            "       â€¢ nhanes_visualizations.zip - Charts for presentations\n",
            "       â€¢ README.txt - Documentation\n",
            "\n",
            "   2ï¸âƒ£  SHOULD SAVE:\n",
            "       â€¢ nhanes_enhanced_data.zip - Full dataset with demographics\n",
            "       â€¢ nhanes_reports.zip - Analysis reports\n",
            "\n",
            "   3ï¸âƒ£  OPTIONAL:\n",
            "       â€¢ nhanes_source_data.zip - Original wide format (can regenerate)\n",
            "\n",
            "ðŸ’¾ TO SAVE TO GOOGLE DRIVE:\n",
            "\n",
            "   Option A: Download individually\n",
            "      â€¢ Right-click each file â†’ Download\n",
            "\n",
            "   Option B: Download entire folder\n",
            "      â€¢ Right-click 'nhanes_diabetes_backup' folder â†’ Download\n",
            "\n",
            "   Option C: Mount Drive and copy\n",
            "      from google.colab import drive\n",
            "      drive.mount('/content/drive')\n",
            "      !cp -r nhanes_diabetes_backup /content/drive/MyDrive/\n",
            "\n",
            "ðŸŽ‰ You can now continue your work anytime by uploading the ZIPs!\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}